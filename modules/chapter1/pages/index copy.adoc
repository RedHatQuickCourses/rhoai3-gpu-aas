= GPU-as-a-Service: Optimizing ROI with Slicing & MIG
:navtitle: GPU Optimization Intro
:imagesdir: ../images
:toc: macro

*Unlock the hidden capacity in your data center. Turn one physical GPU into seven virtual accelerators.*

[abstract]
You have spent the budget on NVIDIA A100s or H100s, but your monitoring dashboards tell a painful story: **Low Utilization.** This module teaches you how to architect a "GPU-as-a-Service" model using Red Hat OpenShift AI (RHOAI), the NVIDIA GPU Operator, and Node Feature Discovery (NFD).

toc::[]

== The Utilization Gap

In a standard Kubernetes environment, a GPU is a "monolithic" resource. If a data scientist requests a GPU for a Jupyter notebook to write code, Kubernetes assigns them the _entire_ physical card.

* **The Cost:** That user might only use 2GB of VRAM and 5% of the compute cores.
* **The Waste:** The remaining 75GB of memory and 95% of compute power sits idle, locked away from other users.
* **The Result:** You run out of allocatable GPUs while your actual hardware sits empty.



== The Solution: Virtualizing the Accelerator

To solve this, we move from "Physical Allocation" to "Virtual Allocation." RHOAI supports two primary methods for slicing GPUs, allowing you to squeeze 4x to 7x more users onto the same hardware.

[cols="1,1", options="header"]
|===
|Strategy A: Time-Slicing (The "Soft" Cut)
|Strategy B: MIG (The "Hard" Cut)

|**What it is:** Software-level oversubscription. The GPU context-switches between processes very quickly.
|**What it is:** Hardware-level partitioning. The GPU is electrically divided into distinct instances.

|**Isolation:** Shared Memory (Low Isolation). One user's OOM error can crash the GPU.
|**Isolation:** Dedicated Memory (High Isolation). Faults are contained within the slice.

|**Best For:** Development, Classrooms, Notebooks, Code Generation.
|**Best For:** Production Inference, Stable Training, Multi-tenant SaaS.
|===

== The Architecture: How It Works

This course will guide you through the three layers required to build this engine:

1.  **The Slicer (NVIDIA GPU Operator):**
    You will configure the `ClusterPolicy` to tell the driver how to present the hardware (e.g., "Split this A100 into 7 slices").

2.  **The Advertiser (Node Feature Discovery - NFD):**
    Once sliced, NFD automatically detects the new virtual resources and labels the nodes.
    * *Before:* `nvidia.com/gpu: 1`
    * *After:* `nvidia.com/mig-1g.5gb: 7`

3.  **The Vending Machine (Hardware Profiles):**
    You will create RHOAI Hardware Profiles that target these specific NFD labels, allowing users to select "Small Slice" from a dropdown menu.

== Technical Preview: The ClusterPolicy

You asked: *How do we actually turn this on?*

In the upcoming "Hands-On" modules, we will be editing the **ClusterPolicy** Custom Resource. Here is a preview of the configuration we will apply to enable these features.

[source,yaml]
----
apiVersion: nvidia.com/v1
kind: ClusterPolicy
metadata:
  name: gpu-cluster-policy
spec:
  # 1. ENABLING MIG (The "Hard" Cut)
  # 'mixed' allows a node to run different sized slices simultaneously
  migStrategy: mixed

  # 2. ENABLING TIME-SLICING (The "Soft" Cut)
  # We reference a ConfigMap that defines how many replicas to create
  devicePlugin:
    config:
      name: time-slicing-config
      default: available  # Applies to GPUs not covered by specific configs
----

[IMPORTANT]
.Prerequisites
====
To complete the labs in this course, you will need:

* **Cluster Admin access** to an OpenShift AI 3.x cluster.
* **NVIDIA GPUs** (Pascal architecture or newer for Time-Slicing; Ampere A100/H100 for MIG).
* The **OpenShift CLI (`oc`)**.
====

== Next Steps

Ready to stop the waste?

* Proceed to **Module 2: Configuring Time-Slicing** for the quickest win.
* Jump to **Module 3: Configuring MIG** for advanced isolation.