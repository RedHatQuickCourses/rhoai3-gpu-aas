<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>GPU-as-a-Service: Optimizing ROI with Slicing &amp; MIG :: GPU as a Service on Red Hat OpenShift AI 3.2</title>
    <meta name="generator" content="Antora 3.1.3">
    <link rel="stylesheet" href="../../../_/css/site.css">
    <script>var uiRootPath = '../../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://www.redhat.com" target="_blank"><img src="../../../_/img/redhat-logo.png" height="40px" alt="Red Hat"></a>
      <a class="navbar-item" style="font-size: 24px; color: white" href="../../..">GPU as a Service on Red Hat OpenShift AI 3.2</a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="https://github.com/RedHatQuickCourses/REPLACEREPONAME/issues" target="_blank">Report Issues</a>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="rhoai-gpu-aas" data-version="1">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../index.html">GPU as a Service on Red Hat OpenShift AI 3.2</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../index.html">Home</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="section1.html">Architecture Deep Dive</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="section2.html">Supply Lab: Time-Slicing</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="section3.html">Governance Lab: Kueue &amp; Profiles</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="section4.html">Governance &amp; Quotas</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="section5.html">Multi-GPU Aggregation</a>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">GPU as a Service on Red Hat OpenShift AI 3.2</span>
    <span class="version">1</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../index.html">GPU as a Service on Red Hat OpenShift AI 3.2</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../index.html">1</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="../index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../index.html">GPU as a Service on Red Hat OpenShift AI 3.2</a></li>
    <li><a href="index%20copy.html">GPU-as-a-Service: Optimizing ROI with Slicing &amp; MIG</a></li>
  </ul>
</nav>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">GPU-as-a-Service: Optimizing ROI with Slicing &amp; MIG</h1>
<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p><strong>Unlock the hidden capacity in your data center. Turn one physical GPU into seven virtual accelerators.</strong></p>
</div>
<div class="quoteblock abstract">
<blockquote>
You have spent the budget on NVIDIA A100s or H100s, but your monitoring dashboards tell a painful story: <strong>Low Utilization.</strong> This module teaches you how to architect a "GPU-as-a-Service" model using Red Hat OpenShift AI (RHOAI), the NVIDIA GPU Operator, and Node Feature Discovery (NFD).
</blockquote>
</div>
<div id="toc" class="toc">
<div id="toctitle" class="title">Table of Contents</div>
<ul class="sectlevel1">
<li><a href="#_the_utilization_gap">The Utilization Gap</a></li>
<li><a href="#_the_solution_virtualizing_the_accelerator">The Solution: Virtualizing the Accelerator</a></li>
<li><a href="#_the_architecture_how_it_works">The Architecture: How It Works</a></li>
<li><a href="#_technical_preview_the_clusterpolicy">Technical Preview: The ClusterPolicy</a></li>
<li><a href="#_next_steps">Next Steps</a></li>
<li><a href="#_the_efficiency_engine_time_slicing">The Efficiency Engine (Time-Slicing)</a>
<ul class="sectlevel1">
<li><a href="#_concept_the_gym_membership_model">Concept: The Gym Membership Model</a></li>
<li><a href="#_step_1_define_the_slicing_configuration">Step 1: Define the Slicing Configuration</a></li>
<li><a href="#_step_2_activate_the_configuration_in_clusterpolicy">Step 2: Activate the Configuration in ClusterPolicy</a></li>
<li><a href="#_step_3_verify_the_magic_node_capacity">Step 3: Verify the "Magic" (Node Capacity)</a></li>
<li><a href="#_step_4_the_hardware_profile">Step 4: The Hardware Profile</a></li>
<li><a href="#_lab_try_it_yourself">Lab: Try it Yourself</a></li>
<li><a href="#_next_steps_2">Next Steps</a></li>
</ul>
</li>
<li><a href="#_the_efficiency_engine_time_slicing_2">The Efficiency Engine (Time-Slicing)</a>
<ul class="sectlevel1">
<li><a href="#_concept_the_gym_membership_model_2">Concept: The Gym Membership Model</a></li>
<li><a href="#_step_1_define_the_slicing_configuration_2">Step 1: Define the Slicing Configuration</a></li>
<li><a href="#_step_2_activate_the_configuration_in_clusterpolicy_2">Step 2: Activate the Configuration in ClusterPolicy</a></li>
<li><a href="#_step_3_verify_the_magic_node_capacity_2">Step 3: Verify the "Magic" (Node Capacity)</a></li>
<li><a href="#_step_4_the_hardware_profile_2">Step 4: The Hardware Profile</a></li>
<li><a href="#_lab_try_it_yourself_2">Lab: Try it Yourself</a></li>
<li><a href="#_next_steps_3">Next Steps</a></li>
</ul>
</li>
<li><a href="#_trust_but_verify_validation_troubleshooting">Trust but Verify (Validation &amp; Troubleshooting)</a>
<ul class="sectlevel1">
<li><a href="#_phase_1_the_flight_check_validation">Phase 1: The Flight Check (Validation)</a>
<ul class="sectlevel2">
<li><a href="#_step_1_verify_the_advertised_capacity">Step 1: Verify the Advertised Capacity</a></li>
<li><a href="#_step_2_verify_the_dashboard_menu">Step 2: Verify the Dashboard "Menu"</a></li>
<li><a href="#_step_3_the_smoke_test_pod_execution">Step 3: The Smoke Test (Pod Execution)</a></li>
</ul>
</li>
<li><a href="#_phase_2_troubleshooting_time_slicing">Phase 2: Troubleshooting Time-Slicing</a>
<ul class="sectlevel2">
<li><a href="#_scenario_a_the_silent_oom">Scenario A: The "Silent OOM"</a></li>
<li><a href="#_scenario_b_replicas_not_showing_up">Scenario B: "Replicas Not Showing Up"</a></li>
</ul>
</li>
<li><a href="#_phase_3_troubleshooting_mig">Phase 3: Troubleshooting MIG</a>
<ul class="sectlevel2">
<li><a href="#_scenario_c_the_stuck_drain">Scenario C: The "Stuck Drain"</a></li>
<li><a href="#_scenario_d_profile_mismatch">Scenario D: "Profile Mismatch"</a></li>
</ul>
</li>
<li><a href="#_summary_of_commands">Summary of Commands</a></li>
<li><a href="#_course_completion">Course Completion</a></li>
</ul>
</li>
<li><a href="#_module_4_the_traffic_controller_governance_with_kueue">Module 4: The Traffic Controller (Governance with Kueue)</a>
<ul class="sectlevel1">
<li><a href="#_the_pinning_vs_queuing_dilemma">The "Pinning" vs. "Queuing" Dilemma</a></li>
<li><a href="#_step_0_enabling_the_service_day_0_config">Step 0: Enabling the Service (Day 0 Config)</a></li>
<li><a href="#_step_1_define_the_resource_flavor_the_physical_layer">Step 1: Define the Resource Flavor (The Physical Layer)</a></li>
<li><a href="#_step_2_define_the_cluster_queue_the_policy_layer">Step 2: Define the Cluster Queue (The Policy Layer)</a></li>
<li><a href="#_step_3_define_the_local_queue_the_bridge">Step 3: Define the Local Queue (The Bridge)</a></li>
<li><a href="#_step_4_the_hardware_profile_the_user_menu">Step 4: The Hardware Profile (The User Menu)</a></li>
<li><a href="#_summary_the_full_supply_chain">Summary: The Full Supply Chain</a></li>
<li><a href="#_next_steps_4">Next Steps</a></li>
</ul>
</li>
<li><a href="#_module_2b_the_heavy_lifters_multi_gpu_aggregation">Module 2b: The Heavy Lifters (Multi-GPU Aggregation)</a>
<ul class="sectlevel1">
<li><a href="#_concept_the_voltron_strategy">Concept: The "Voltron" Strategy</a></li>
<li><a href="#_step_1_the_basic_bundle_profile">Step 1: The Basic "Bundle" Profile</a></li>
<li><a href="#_step_2_the_topology_trap_critical_warning">Step 2: The Topology Trap (Critical Warning)</a></li>
<li><a href="#_step_3_aggregation_with_kueue_governance">Step 3: Aggregation with Kueue (Governance)</a></li>
<li><a href="#_lab_create_a_heavy_lifter">Lab: Create a "Heavy Lifter"</a></li>
<li><a href="#_summary_the_scaling_up_strategy">Summary: The "Scaling Up" Strategy</a></li>
</ul>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_the_utilization_gap"><a class="anchor" href="#_the_utilization_gap"></a>The Utilization Gap</h2>
<div class="sectionbody">
<div class="paragraph">
<p>In a standard Kubernetes environment, a GPU is a "monolithic" resource. If a data scientist requests a GPU for a Jupyter notebook to write code, Kubernetes assigns them the <em>entire</em> physical card.</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>The Cost:</strong> That user might only use 2GB of VRAM and 5% of the compute cores.</p>
</li>
<li>
<p><strong>The Waste:</strong> The remaining 75GB of memory and 95% of compute power sits idle, locked away from other users.</p>
</li>
<li>
<p><strong>The Result:</strong> You run out of allocatable GPUs while your actual hardware sits empty.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_the_solution_virtualizing_the_accelerator"><a class="anchor" href="#_the_solution_virtualizing_the_accelerator"></a>The Solution: Virtualizing the Accelerator</h2>
<div class="sectionbody">
<div class="paragraph">
<p>To solve this, we move from "Physical Allocation" to "Virtual Allocation." RHOAI supports two primary methods for slicing GPUs, allowing you to squeeze 4x to 7x more users onto the same hardware.</p>
</div>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 50%;">
<col style="width: 50%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Strategy A: Time-Slicing (The "Soft" Cut)</th>
<th class="tableblock halign-left valign-top">Strategy B: MIG (The "Hard" Cut)</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>What it is:</strong> Software-level oversubscription. The GPU context-switches between processes very quickly.</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>What it is:</strong> Hardware-level partitioning. The GPU is electrically divided into distinct instances.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Isolation:</strong> Shared Memory (Low Isolation). One user&#8217;s OOM error can crash the GPU.</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Isolation:</strong> Dedicated Memory (High Isolation). Faults are contained within the slice.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Best For:</strong> Development, Classrooms, Notebooks, Code Generation.</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Best For:</strong> Production Inference, Stable Training, Multi-tenant SaaS.</p></td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="sect1">
<h2 id="_the_architecture_how_it_works"><a class="anchor" href="#_the_architecture_how_it_works"></a>The Architecture: How It Works</h2>
<div class="sectionbody">
<div class="paragraph">
<p>This course will guide you through the three layers required to build this engine:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong>The Slicer (NVIDIA GPU Operator):</strong>
You will configure the <code>ClusterPolicy</code> to tell the driver how to present the hardware (e.g., "Split this A100 into 7 slices").</p>
</li>
<li>
<p><strong>The Advertiser (Node Feature Discovery - NFD):</strong>
Once sliced, NFD automatically detects the new virtual resources and labels the nodes.</p>
<div class="ulist">
<ul>
<li>
<p><strong>Before:</strong> <code>nvidia.com/gpu: 1</code></p>
</li>
<li>
<p><strong>After:</strong> <code>nvidia.com/mig-1g.5gb: 7</code></p>
</li>
</ul>
</div>
</li>
<li>
<p><strong>The Vending Machine (Hardware Profiles):</strong>
You will create RHOAI Hardware Profiles that target these specific NFD labels, allowing users to select "Small Slice" from a dropdown menu.</p>
</li>
</ol>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_technical_preview_the_clusterpolicy"><a class="anchor" href="#_technical_preview_the_clusterpolicy"></a>Technical Preview: The ClusterPolicy</h2>
<div class="sectionbody">
<div class="paragraph">
<p>You asked: <strong>How do we actually turn this on?</strong></p>
</div>
<div class="paragraph">
<p>In the upcoming "Hands-On" modules, we will be editing the <strong>ClusterPolicy</strong> Custom Resource. Here is a preview of the configuration we will apply to enable these features.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: nvidia.com/v1
kind: ClusterPolicy
metadata:
  name: gpu-cluster-policy
spec:
  # 1. ENABLING MIG (The "Hard" Cut)
  # 'mixed' allows a node to run different sized slices simultaneously
  migStrategy: mixed

  # 2. ENABLING TIME-SLICING (The "Soft" Cut)
  # We reference a ConfigMap that defines how many replicas to create
  devicePlugin:
    config:
      name: time-slicing-config
      default: available  # Applies to GPUs not covered by specific configs</code></pre>
</div>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
<div class="title">Prerequisites</div>
<div class="paragraph">
<p>To complete the labs in this course, you will need:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Cluster Admin access</strong> to an OpenShift AI 3.x cluster.</p>
</li>
<li>
<p><strong>NVIDIA GPUs</strong> (Pascal architecture or newer for Time-Slicing; Ampere A100/H100 for MIG).</p>
</li>
<li>
<p>The <strong>OpenShift CLI (<code>oc</code>)</strong>.</p>
</li>
</ul>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_next_steps"><a class="anchor" href="#_next_steps"></a>Next Steps</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Ready to stop the waste?</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Proceed to <strong>Module 2: Configuring Time-Slicing</strong> for the quickest win.</p>
</li>
<li>
<p>Jump to <strong>Module 3: Configuring MIG</strong> for advanced isolation.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>=========_++</p>
</div>
</div>
</div>
<h1 id="_the_efficiency_engine_time_slicing" class="sect0"><a class="anchor" href="#_the_efficiency_engine_time_slicing"></a>The Efficiency Engine (Time-Slicing)</h1>
<div class="paragraph">
<p><strong>How to oversubscribe your hardware and solve the "No GPUs Available" crisis.</strong></p>
</div>
<div class="quoteblock abstract">
<blockquote>
Time-slicing is the quickest way to increase density in your cluster. By enabling software-level interleaving, you can allow multiple workloads to share a single GPU. This module guides you through the three-step configuration process: The Map, The Policy, and The Label.
</blockquote>
</div>
<div id="toc" class="toc">
<div id="toctitle" class="title">Table of Contents</div>
<ul class="sectlevel1">
<li><a href="#_the_utilization_gap">The Utilization Gap</a></li>
<li><a href="#_the_solution_virtualizing_the_accelerator">The Solution: Virtualizing the Accelerator</a></li>
<li><a href="#_the_architecture_how_it_works">The Architecture: How It Works</a></li>
<li><a href="#_technical_preview_the_clusterpolicy">Technical Preview: The ClusterPolicy</a></li>
<li><a href="#_next_steps">Next Steps</a></li>
<li><a href="#_the_efficiency_engine_time_slicing">The Efficiency Engine (Time-Slicing)</a>
<ul class="sectlevel1">
<li><a href="#_concept_the_gym_membership_model">Concept: The Gym Membership Model</a></li>
<li><a href="#_step_1_define_the_slicing_configuration">Step 1: Define the Slicing Configuration</a></li>
<li><a href="#_step_2_activate_the_configuration_in_clusterpolicy">Step 2: Activate the Configuration in ClusterPolicy</a></li>
<li><a href="#_step_3_verify_the_magic_node_capacity">Step 3: Verify the "Magic" (Node Capacity)</a></li>
<li><a href="#_step_4_the_hardware_profile">Step 4: The Hardware Profile</a></li>
<li><a href="#_lab_try_it_yourself">Lab: Try it Yourself</a></li>
<li><a href="#_next_steps_2">Next Steps</a></li>
</ul>
</li>
<li><a href="#_the_efficiency_engine_time_slicing_2">The Efficiency Engine (Time-Slicing)</a>
<ul class="sectlevel1">
<li><a href="#_concept_the_gym_membership_model_2">Concept: The Gym Membership Model</a></li>
<li><a href="#_step_1_define_the_slicing_configuration_2">Step 1: Define the Slicing Configuration</a></li>
<li><a href="#_step_2_activate_the_configuration_in_clusterpolicy_2">Step 2: Activate the Configuration in ClusterPolicy</a></li>
<li><a href="#_step_3_verify_the_magic_node_capacity_2">Step 3: Verify the "Magic" (Node Capacity)</a></li>
<li><a href="#_step_4_the_hardware_profile_2">Step 4: The Hardware Profile</a></li>
<li><a href="#_lab_try_it_yourself_2">Lab: Try it Yourself</a></li>
<li><a href="#_next_steps_3">Next Steps</a></li>
</ul>
</li>
<li><a href="#_trust_but_verify_validation_troubleshooting">Trust but Verify (Validation &amp; Troubleshooting)</a>
<ul class="sectlevel1">
<li><a href="#_phase_1_the_flight_check_validation">Phase 1: The Flight Check (Validation)</a>
<ul class="sectlevel2">
<li><a href="#_step_1_verify_the_advertised_capacity">Step 1: Verify the Advertised Capacity</a></li>
<li><a href="#_step_2_verify_the_dashboard_menu">Step 2: Verify the Dashboard "Menu"</a></li>
<li><a href="#_step_3_the_smoke_test_pod_execution">Step 3: The Smoke Test (Pod Execution)</a></li>
</ul>
</li>
<li><a href="#_phase_2_troubleshooting_time_slicing">Phase 2: Troubleshooting Time-Slicing</a>
<ul class="sectlevel2">
<li><a href="#_scenario_a_the_silent_oom">Scenario A: The "Silent OOM"</a></li>
<li><a href="#_scenario_b_replicas_not_showing_up">Scenario B: "Replicas Not Showing Up"</a></li>
</ul>
</li>
<li><a href="#_phase_3_troubleshooting_mig">Phase 3: Troubleshooting MIG</a>
<ul class="sectlevel2">
<li><a href="#_scenario_c_the_stuck_drain">Scenario C: The "Stuck Drain"</a></li>
<li><a href="#_scenario_d_profile_mismatch">Scenario D: "Profile Mismatch"</a></li>
</ul>
</li>
<li><a href="#_summary_of_commands">Summary of Commands</a></li>
<li><a href="#_course_completion">Course Completion</a></li>
</ul>
</li>
<li><a href="#_module_4_the_traffic_controller_governance_with_kueue">Module 4: The Traffic Controller (Governance with Kueue)</a>
<ul class="sectlevel1">
<li><a href="#_the_pinning_vs_queuing_dilemma">The "Pinning" vs. "Queuing" Dilemma</a></li>
<li><a href="#_step_0_enabling_the_service_day_0_config">Step 0: Enabling the Service (Day 0 Config)</a></li>
<li><a href="#_step_1_define_the_resource_flavor_the_physical_layer">Step 1: Define the Resource Flavor (The Physical Layer)</a></li>
<li><a href="#_step_2_define_the_cluster_queue_the_policy_layer">Step 2: Define the Cluster Queue (The Policy Layer)</a></li>
<li><a href="#_step_3_define_the_local_queue_the_bridge">Step 3: Define the Local Queue (The Bridge)</a></li>
<li><a href="#_step_4_the_hardware_profile_the_user_menu">Step 4: The Hardware Profile (The User Menu)</a></li>
<li><a href="#_summary_the_full_supply_chain">Summary: The Full Supply Chain</a></li>
<li><a href="#_next_steps_4">Next Steps</a></li>
</ul>
</li>
<li><a href="#_module_2b_the_heavy_lifters_multi_gpu_aggregation">Module 2b: The Heavy Lifters (Multi-GPU Aggregation)</a>
<ul class="sectlevel1">
<li><a href="#_concept_the_voltron_strategy">Concept: The "Voltron" Strategy</a></li>
<li><a href="#_step_1_the_basic_bundle_profile">Step 1: The Basic "Bundle" Profile</a></li>
<li><a href="#_step_2_the_topology_trap_critical_warning">Step 2: The Topology Trap (Critical Warning)</a></li>
<li><a href="#_step_3_aggregation_with_kueue_governance">Step 3: Aggregation with Kueue (Governance)</a></li>
<li><a href="#_lab_create_a_heavy_lifter">Lab: Create a "Heavy Lifter"</a></li>
<li><a href="#_summary_the_scaling_up_strategy">Summary: The "Scaling Up" Strategy</a></li>
</ul>
</li>
</ul>
</div>
<div class="sect1">
<h2 id="_concept_the_gym_membership_model"><a class="anchor" href="#_concept_the_gym_membership_model"></a>Concept: The Gym Membership Model</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Think of Time-Slicing like a gym. You might have 100 members but only 10 treadmills. It works because not everyone runs at the exact same second.</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Without Time-Slicing:</strong> When a user opens a Jupyter Notebook, they "lock the door" to the gym. No one else can enter, even if the user is just reading documentation.</p>
</li>
<li>
<p><strong>With Time-Slicing:</strong> The door is unlocked. Multiple users can enter. The GPU (the treadmill) rapidly switches between them.</p>
</li>
</ul>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
<div class="title">The Safety Warning</div>
<div class="paragraph">
<p><strong>Shared Memory:</strong> Time-slicing provides <strong>compute</strong> isolation (via context switching) but <strong>not memory isolation</strong>. All users share the same VRAM pool.</p>
</div>
<div class="paragraph">
<p>+
 * <strong>Risk:</strong> If User A runs a massive job that consumes 100% of the VRAM, User B&#8217;s process will crash (OOM).</p>
</div>
<div class="paragraph">
<p>+
 * <strong>Mitigation:</strong> Use this strategy for Development, Notebooks, and Education—not for heavy production training.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_step_1_define_the_slicing_configuration"><a class="anchor" href="#_step_1_define_the_slicing_configuration"></a>Step 1: Define the Slicing Configuration</h2>
<div class="sectionbody">
<div class="paragraph">
<p>First, we tell the NVIDIA GPU Operator <strong>how</strong> to slice the cards. We do this with a <code>ConfigMap</code>.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: time-slicing-config
  namespace: nvidia-gpu-operator
data:
  any: |-  <i class="conum" data-value="1"></i><b>(1)</b>
    version: v1
    sharing:
      timeSlicing:
        resources:
        - name: nvidia.com/gpu
          replicas: 4  <i class="conum" data-value="2"></i><b>(2)</b></code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td><strong>The Config Key:</strong> We use <code>any</code> to apply this generic rule to any NVIDIA GPU. You can also use specific product names (e.g., <code>tesla-t4</code>, <code>a100</code>) to create different rules for different cards.</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td><strong>Replicas:</strong> This is the magic number. We are telling the driver to advertise 4 "virtual" GPUs for every 1 physical GPU.</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_step_2_activate_the_configuration_in_clusterpolicy"><a class="anchor" href="#_step_2_activate_the_configuration_in_clusterpolicy"></a>Step 2: Activate the Configuration in ClusterPolicy</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Merely creating the ConfigMap does nothing. We must instruct the GPU Operator to read it.</p>
</div>
<div class="paragraph">
<p>We patch the <code>ClusterPolicy</code> to reference our new map.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: nvidia.com/v1
kind: ClusterPolicy
metadata:
  name: gpu-cluster-policy
spec:
  migStrategy: none  <i class="conum" data-value="1"></i><b>(1)</b>
  devicePlugin:
    config:
      name: time-slicing-config  <i class="conum" data-value="2"></i><b>(2)</b>
      default: any  <i class="conum" data-value="3"></i><b>(3)</b></code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td><strong>Disable MIG:</strong> Time-slicing and MIG cannot usually coexist on the same node. We ensure MIG is off.</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td><strong>Reference:</strong> Points to the ConfigMap we created in Step 1.</td>
</tr>
<tr>
<td><i class="conum" data-value="3"></i><b>3</b></td>
<td><strong>Default Strategy:</strong> Tells the operator to apply the configuration keyed as <code>any</code> (from Step 1) to all GPUs unless specified otherwise.</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_step_3_verify_the_magic_node_capacity"><a class="anchor" href="#_step_3_verify_the_magic_node_capacity"></a>Step 3: Verify the "Magic" (Node Capacity)</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Once the Operator processes the change, the Device Plugin will restart. You can verify the success by looking at the Node&#8217;s capacity.</p>
</div>
<div class="paragraph">
<p>Run the following command:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">oc describe node &lt;your-gpu-node&gt; | grep "nvidia.com/gpu"</code></pre>
</div>
</div>
<table class="tableblock frame-all grid-all stretch">
<caption class="title">Table 1. Output Comparison</caption>
<colgroup>
<col style="width: 50%;">
<col style="width: 50%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Before</th>
<th class="tableblock halign-left valign-top">After</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Capacity: <code>nvidia.com/gpu: 1</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Capacity: <code>nvidia.com/gpu: 4</code></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Allocatable: <code>nvidia.com/gpu: 1</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Allocatable: <code>nvidia.com/gpu: 4</code></p></td>
</tr>
</tbody>
</table>
<div class="paragraph">
<p>The node now claims to have 4 GPUs. Kubernetes doesn&#8217;t know they are fake; it just schedules pods until the counter hits 0.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_step_4_the_hardware_profile"><a class="anchor" href="#_step_4_the_hardware_profile"></a>Step 4: The Hardware Profile</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Now that the capacity exists, how do users consume it?</p>
</div>
<div class="paragraph">
<p>With Time-Slicing, the resource name remains <code>nvidia.com/gpu</code>. Therefore, you do <strong>not</strong> need a specialized identifier in your Hardware Profile. You simply create a profile for "Shared GPU".</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: dashboard.opendatahub.io/v1alpha1
kind: HardwareProfile
metadata:
  name: nvidia-gpu-shared
  namespace: redhat-ods-applications # Global profile
spec:
  displayName: "NVIDIA GPU (Shared)"
  description: "Best for interactive workbooks and coding. Memory is shared with other users."
  identifiers:
    - identifier: nvidia.com/gpu
      count: 1  <i class="conum" data-value="1"></i><b>(1)</b>
  tolerations:
    - key: "nvidia.com/gpu"
      operator: "Exists"
      effect: "NoSchedule"</code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td><strong>The User Request:</strong> The user still asks for "1 GPU". But because the node now has 4 to give, 4 users can successfully launch this profile on the same node.</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_lab_try_it_yourself"><a class="anchor" href="#_lab_try_it_yourself"></a>Lab: Try it Yourself</h2>
<div class="sectionbody">
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Create the <code>time-slicing-config</code> ConfigMap.</p>
</li>
<li>
<p>Edit the <code>gpu-cluster-policy</code> to reference it.</p>
</li>
<li>
<p>Watch the pods in <code>nvidia-gpu-operator</code> namespace restart.</p>
</li>
<li>
<p>Verify <code>oc describe node</code> shows increased capacity.</p>
</li>
<li>
<p>Create two Notebooks using the "Shared" profile and verify they land on the same node.</p>
</li>
</ol>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_next_steps_2"><a class="anchor" href="#_next_steps_2"></a>Next Steps</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Time-slicing is great for density, but what if you need <strong>guaranteed performance</strong> and <strong>strict isolation</strong>?
Proceed to <strong>Module 3: Configuring MIG</strong> to learn the "Hard" partition strategy.</p>
</div>
<div class="paragraph">
<p>===++======+=====</p>
</div>
</div>
</div>
<h1 id="_the_efficiency_engine_time_slicing_2" class="sect0"><a class="anchor" href="#_the_efficiency_engine_time_slicing_2"></a>The Efficiency Engine (Time-Slicing)</h1>
<div class="paragraph">
<p><strong>How to oversubscribe your hardware and solve the "No GPUs Available" crisis.</strong></p>
</div>
<div class="quoteblock abstract">
<blockquote>
Time-slicing is the quickest way to increase density in your cluster. By enabling software-level interleaving, you can allow multiple workloads to share a single GPU. This module guides you through the three-step configuration process: The Map, The Policy, and The Label.
</blockquote>
</div>
<div id="toc" class="toc">
<div id="toctitle" class="title">Table of Contents</div>
<ul class="sectlevel1">
<li><a href="#_the_utilization_gap">The Utilization Gap</a></li>
<li><a href="#_the_solution_virtualizing_the_accelerator">The Solution: Virtualizing the Accelerator</a></li>
<li><a href="#_the_architecture_how_it_works">The Architecture: How It Works</a></li>
<li><a href="#_technical_preview_the_clusterpolicy">Technical Preview: The ClusterPolicy</a></li>
<li><a href="#_next_steps">Next Steps</a></li>
<li><a href="#_the_efficiency_engine_time_slicing">The Efficiency Engine (Time-Slicing)</a>
<ul class="sectlevel1">
<li><a href="#_concept_the_gym_membership_model">Concept: The Gym Membership Model</a></li>
<li><a href="#_step_1_define_the_slicing_configuration">Step 1: Define the Slicing Configuration</a></li>
<li><a href="#_step_2_activate_the_configuration_in_clusterpolicy">Step 2: Activate the Configuration in ClusterPolicy</a></li>
<li><a href="#_step_3_verify_the_magic_node_capacity">Step 3: Verify the "Magic" (Node Capacity)</a></li>
<li><a href="#_step_4_the_hardware_profile">Step 4: The Hardware Profile</a></li>
<li><a href="#_lab_try_it_yourself">Lab: Try it Yourself</a></li>
<li><a href="#_next_steps_2">Next Steps</a></li>
</ul>
</li>
<li><a href="#_the_efficiency_engine_time_slicing_2">The Efficiency Engine (Time-Slicing)</a>
<ul class="sectlevel1">
<li><a href="#_concept_the_gym_membership_model_2">Concept: The Gym Membership Model</a></li>
<li><a href="#_step_1_define_the_slicing_configuration_2">Step 1: Define the Slicing Configuration</a></li>
<li><a href="#_step_2_activate_the_configuration_in_clusterpolicy_2">Step 2: Activate the Configuration in ClusterPolicy</a></li>
<li><a href="#_step_3_verify_the_magic_node_capacity_2">Step 3: Verify the "Magic" (Node Capacity)</a></li>
<li><a href="#_step_4_the_hardware_profile_2">Step 4: The Hardware Profile</a></li>
<li><a href="#_lab_try_it_yourself_2">Lab: Try it Yourself</a></li>
<li><a href="#_next_steps_3">Next Steps</a></li>
</ul>
</li>
<li><a href="#_trust_but_verify_validation_troubleshooting">Trust but Verify (Validation &amp; Troubleshooting)</a>
<ul class="sectlevel1">
<li><a href="#_phase_1_the_flight_check_validation">Phase 1: The Flight Check (Validation)</a>
<ul class="sectlevel2">
<li><a href="#_step_1_verify_the_advertised_capacity">Step 1: Verify the Advertised Capacity</a></li>
<li><a href="#_step_2_verify_the_dashboard_menu">Step 2: Verify the Dashboard "Menu"</a></li>
<li><a href="#_step_3_the_smoke_test_pod_execution">Step 3: The Smoke Test (Pod Execution)</a></li>
</ul>
</li>
<li><a href="#_phase_2_troubleshooting_time_slicing">Phase 2: Troubleshooting Time-Slicing</a>
<ul class="sectlevel2">
<li><a href="#_scenario_a_the_silent_oom">Scenario A: The "Silent OOM"</a></li>
<li><a href="#_scenario_b_replicas_not_showing_up">Scenario B: "Replicas Not Showing Up"</a></li>
</ul>
</li>
<li><a href="#_phase_3_troubleshooting_mig">Phase 3: Troubleshooting MIG</a>
<ul class="sectlevel2">
<li><a href="#_scenario_c_the_stuck_drain">Scenario C: The "Stuck Drain"</a></li>
<li><a href="#_scenario_d_profile_mismatch">Scenario D: "Profile Mismatch"</a></li>
</ul>
</li>
<li><a href="#_summary_of_commands">Summary of Commands</a></li>
<li><a href="#_course_completion">Course Completion</a></li>
</ul>
</li>
<li><a href="#_module_4_the_traffic_controller_governance_with_kueue">Module 4: The Traffic Controller (Governance with Kueue)</a>
<ul class="sectlevel1">
<li><a href="#_the_pinning_vs_queuing_dilemma">The "Pinning" vs. "Queuing" Dilemma</a></li>
<li><a href="#_step_0_enabling_the_service_day_0_config">Step 0: Enabling the Service (Day 0 Config)</a></li>
<li><a href="#_step_1_define_the_resource_flavor_the_physical_layer">Step 1: Define the Resource Flavor (The Physical Layer)</a></li>
<li><a href="#_step_2_define_the_cluster_queue_the_policy_layer">Step 2: Define the Cluster Queue (The Policy Layer)</a></li>
<li><a href="#_step_3_define_the_local_queue_the_bridge">Step 3: Define the Local Queue (The Bridge)</a></li>
<li><a href="#_step_4_the_hardware_profile_the_user_menu">Step 4: The Hardware Profile (The User Menu)</a></li>
<li><a href="#_summary_the_full_supply_chain">Summary: The Full Supply Chain</a></li>
<li><a href="#_next_steps_4">Next Steps</a></li>
</ul>
</li>
<li><a href="#_module_2b_the_heavy_lifters_multi_gpu_aggregation">Module 2b: The Heavy Lifters (Multi-GPU Aggregation)</a>
<ul class="sectlevel1">
<li><a href="#_concept_the_voltron_strategy">Concept: The "Voltron" Strategy</a></li>
<li><a href="#_step_1_the_basic_bundle_profile">Step 1: The Basic "Bundle" Profile</a></li>
<li><a href="#_step_2_the_topology_trap_critical_warning">Step 2: The Topology Trap (Critical Warning)</a></li>
<li><a href="#_step_3_aggregation_with_kueue_governance">Step 3: Aggregation with Kueue (Governance)</a></li>
<li><a href="#_lab_create_a_heavy_lifter">Lab: Create a "Heavy Lifter"</a></li>
<li><a href="#_summary_the_scaling_up_strategy">Summary: The "Scaling Up" Strategy</a></li>
</ul>
</li>
</ul>
</div>
<div class="sect1">
<h2 id="_concept_the_gym_membership_model_2"><a class="anchor" href="#_concept_the_gym_membership_model_2"></a>Concept: The Gym Membership Model</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Think of Time-Slicing like a gym. You might have 100 members but only 10 treadmills. It works because not everyone runs at the exact same second.</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Without Time-Slicing:</strong> When a user opens a Jupyter Notebook, they "lock the door" to the gym. No one else can enter, even if the user is just reading documentation.</p>
</li>
<li>
<p><strong>With Time-Slicing:</strong> The door is unlocked. Multiple users can enter. The GPU (the treadmill) rapidly switches between them.</p>
</li>
</ul>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
<div class="title">The Safety Warning</div>
<div class="paragraph">
<p><strong>Shared Memory:</strong> Time-slicing provides <strong>compute</strong> isolation (via context switching) but <strong>not memory isolation</strong>. All users share the same VRAM pool.</p>
</div>
<div class="paragraph">
<p>+
 * <strong>Risk:</strong> If User A runs a massive job that consumes 100% of the VRAM, User B&#8217;s process will crash (OOM).</p>
</div>
<div class="paragraph">
<p>+
 * <strong>Mitigation:</strong> Use this strategy for Development, Notebooks, and Education—not for heavy production training.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_step_1_define_the_slicing_configuration_2"><a class="anchor" href="#_step_1_define_the_slicing_configuration_2"></a>Step 1: Define the Slicing Configuration</h2>
<div class="sectionbody">
<div class="paragraph">
<p>First, we tell the NVIDIA GPU Operator <strong>how</strong> to slice the cards. We do this with a <code>ConfigMap</code>.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: time-slicing-config
  namespace: nvidia-gpu-operator
data:
  any: |-  <i class="conum" data-value="1"></i><b>(1)</b>
    version: v1
    sharing:
      timeSlicing:
        resources:
        - name: nvidia.com/gpu
          replicas: 4  <i class="conum" data-value="2"></i><b>(2)</b></code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td><strong>The Config Key:</strong> We use <code>any</code> to apply this generic rule to any NVIDIA GPU. You can also use specific product names (e.g., <code>tesla-t4</code>, <code>a100</code>) to create different rules for different cards.</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td><strong>Replicas:</strong> This is the magic number. We are telling the driver to advertise 4 "virtual" GPUs for every 1 physical GPU.</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_step_2_activate_the_configuration_in_clusterpolicy_2"><a class="anchor" href="#_step_2_activate_the_configuration_in_clusterpolicy_2"></a>Step 2: Activate the Configuration in ClusterPolicy</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Merely creating the ConfigMap does nothing. We must instruct the GPU Operator to read it.</p>
</div>
<div class="paragraph">
<p>We patch the <code>ClusterPolicy</code> to reference our new map.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: nvidia.com/v1
kind: ClusterPolicy
metadata:
  name: gpu-cluster-policy
spec:
  migStrategy: none  <i class="conum" data-value="1"></i><b>(1)</b>
  devicePlugin:
    config:
      name: time-slicing-config  <i class="conum" data-value="2"></i><b>(2)</b>
      default: any  <i class="conum" data-value="3"></i><b>(3)</b></code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td><strong>Disable MIG:</strong> Time-slicing and MIG cannot usually coexist on the same node. We ensure MIG is off.</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td><strong>Reference:</strong> Points to the ConfigMap we created in Step 1.</td>
</tr>
<tr>
<td><i class="conum" data-value="3"></i><b>3</b></td>
<td><strong>Default Strategy:</strong> Tells the operator to apply the configuration keyed as <code>any</code> (from Step 1) to all GPUs unless specified otherwise.</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_step_3_verify_the_magic_node_capacity_2"><a class="anchor" href="#_step_3_verify_the_magic_node_capacity_2"></a>Step 3: Verify the "Magic" (Node Capacity)</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Once the Operator processes the change, the Device Plugin will restart. You can verify the success by looking at the Node&#8217;s capacity.</p>
</div>
<div class="paragraph">
<p>Run the following command:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">oc describe node &lt;your-gpu-node&gt; | grep "nvidia.com/gpu"</code></pre>
</div>
</div>
<table class="tableblock frame-all grid-all stretch">
<caption class="title">Table 2. Output Comparison</caption>
<colgroup>
<col style="width: 50%;">
<col style="width: 50%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Before</th>
<th class="tableblock halign-left valign-top">After</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Capacity: <code>nvidia.com/gpu: 1</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Capacity: <code>nvidia.com/gpu: 4</code></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Allocatable: <code>nvidia.com/gpu: 1</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Allocatable: <code>nvidia.com/gpu: 4</code></p></td>
</tr>
</tbody>
</table>
<div class="paragraph">
<p>The node now claims to have 4 GPUs. Kubernetes doesn&#8217;t know they are fake; it just schedules pods until the counter hits 0.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_step_4_the_hardware_profile_2"><a class="anchor" href="#_step_4_the_hardware_profile_2"></a>Step 4: The Hardware Profile</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Now that the capacity exists, how do users consume it?</p>
</div>
<div class="paragraph">
<p>With Time-Slicing, the resource name remains <code>nvidia.com/gpu</code>. Therefore, you do <strong>not</strong> need a specialized identifier in your Hardware Profile. You simply create a profile for "Shared GPU".</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: dashboard.opendatahub.io/v1alpha1
kind: HardwareProfile
metadata:
  name: nvidia-gpu-shared
  namespace: redhat-ods-applications # Global profile
spec:
  displayName: "NVIDIA GPU (Shared)"
  description: "Best for interactive workbooks and coding. Memory is shared with other users."
  identifiers:
    - identifier: nvidia.com/gpu
      count: 1  <i class="conum" data-value="1"></i><b>(1)</b>
  tolerations:
    - key: "nvidia.com/gpu"
      operator: "Exists"
      effect: "NoSchedule"</code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td><strong>The User Request:</strong> The user still asks for "1 GPU". But because the node now has 4 to give, 4 users can successfully launch this profile on the same node.</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_lab_try_it_yourself_2"><a class="anchor" href="#_lab_try_it_yourself_2"></a>Lab: Try it Yourself</h2>
<div class="sectionbody">
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Create the <code>time-slicing-config</code> ConfigMap.</p>
</li>
<li>
<p>Edit the <code>gpu-cluster-policy</code> to reference it.</p>
</li>
<li>
<p>Watch the pods in <code>nvidia-gpu-operator</code> namespace restart.</p>
</li>
<li>
<p>Verify <code>oc describe node</code> shows increased capacity.</p>
</li>
<li>
<p>Create two Notebooks using the "Shared" profile and verify they land on the same node.</p>
</li>
</ol>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_next_steps_3"><a class="anchor" href="#_next_steps_3"></a>Next Steps</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Time-slicing is great for density, but what if you need <strong>guaranteed performance</strong> and <strong>strict isolation</strong>?
Proceed to <strong>Module 3: Configuring MIG</strong> to learn the "Hard" partition strategy.</p>
</div>
<div class="paragraph">
<p>====------======</p>
</div>
</div>
</div>
<h1 id="_trust_but_verify_validation_troubleshooting" class="sect0"><a class="anchor" href="#_trust_but_verify_validation_troubleshooting"></a>Trust but Verify (Validation &amp; Troubleshooting)</h1>
<div class="paragraph">
<p><strong>How to confirm your allocation engine is running and fix common "GPU Starvation" errors.</strong></p>
</div>
<div class="quoteblock abstract">
<blockquote>
You have applied the ConfigMaps and labeled the nodes. But is the system actually serving slices? This module provides the "Flight Check" procedures to validate your GPU-as-a-Service implementation and troubleshoot the two most common failure modes: The Silent OOM (Time-Slicing) and The Stuck Drain (MIG).
</blockquote>
</div>
<div id="toc" class="toc">
<div id="toctitle" class="title">Table of Contents</div>
<ul class="sectlevel1">
<li><a href="#_the_utilization_gap">The Utilization Gap</a></li>
<li><a href="#_the_solution_virtualizing_the_accelerator">The Solution: Virtualizing the Accelerator</a></li>
<li><a href="#_the_architecture_how_it_works">The Architecture: How It Works</a></li>
<li><a href="#_technical_preview_the_clusterpolicy">Technical Preview: The ClusterPolicy</a></li>
<li><a href="#_next_steps">Next Steps</a></li>
<li><a href="#_the_efficiency_engine_time_slicing">The Efficiency Engine (Time-Slicing)</a>
<ul class="sectlevel1">
<li><a href="#_concept_the_gym_membership_model">Concept: The Gym Membership Model</a></li>
<li><a href="#_step_1_define_the_slicing_configuration">Step 1: Define the Slicing Configuration</a></li>
<li><a href="#_step_2_activate_the_configuration_in_clusterpolicy">Step 2: Activate the Configuration in ClusterPolicy</a></li>
<li><a href="#_step_3_verify_the_magic_node_capacity">Step 3: Verify the "Magic" (Node Capacity)</a></li>
<li><a href="#_step_4_the_hardware_profile">Step 4: The Hardware Profile</a></li>
<li><a href="#_lab_try_it_yourself">Lab: Try it Yourself</a></li>
<li><a href="#_next_steps_2">Next Steps</a></li>
</ul>
</li>
<li><a href="#_the_efficiency_engine_time_slicing_2">The Efficiency Engine (Time-Slicing)</a>
<ul class="sectlevel1">
<li><a href="#_concept_the_gym_membership_model_2">Concept: The Gym Membership Model</a></li>
<li><a href="#_step_1_define_the_slicing_configuration_2">Step 1: Define the Slicing Configuration</a></li>
<li><a href="#_step_2_activate_the_configuration_in_clusterpolicy_2">Step 2: Activate the Configuration in ClusterPolicy</a></li>
<li><a href="#_step_3_verify_the_magic_node_capacity_2">Step 3: Verify the "Magic" (Node Capacity)</a></li>
<li><a href="#_step_4_the_hardware_profile_2">Step 4: The Hardware Profile</a></li>
<li><a href="#_lab_try_it_yourself_2">Lab: Try it Yourself</a></li>
<li><a href="#_next_steps_3">Next Steps</a></li>
</ul>
</li>
<li><a href="#_trust_but_verify_validation_troubleshooting">Trust but Verify (Validation &amp; Troubleshooting)</a>
<ul class="sectlevel1">
<li><a href="#_phase_1_the_flight_check_validation">Phase 1: The Flight Check (Validation)</a>
<ul class="sectlevel2">
<li><a href="#_step_1_verify_the_advertised_capacity">Step 1: Verify the Advertised Capacity</a></li>
<li><a href="#_step_2_verify_the_dashboard_menu">Step 2: Verify the Dashboard "Menu"</a></li>
<li><a href="#_step_3_the_smoke_test_pod_execution">Step 3: The Smoke Test (Pod Execution)</a></li>
</ul>
</li>
<li><a href="#_phase_2_troubleshooting_time_slicing">Phase 2: Troubleshooting Time-Slicing</a>
<ul class="sectlevel2">
<li><a href="#_scenario_a_the_silent_oom">Scenario A: The "Silent OOM"</a></li>
<li><a href="#_scenario_b_replicas_not_showing_up">Scenario B: "Replicas Not Showing Up"</a></li>
</ul>
</li>
<li><a href="#_phase_3_troubleshooting_mig">Phase 3: Troubleshooting MIG</a>
<ul class="sectlevel2">
<li><a href="#_scenario_c_the_stuck_drain">Scenario C: The "Stuck Drain"</a></li>
<li><a href="#_scenario_d_profile_mismatch">Scenario D: "Profile Mismatch"</a></li>
</ul>
</li>
<li><a href="#_summary_of_commands">Summary of Commands</a></li>
<li><a href="#_course_completion">Course Completion</a></li>
</ul>
</li>
<li><a href="#_module_4_the_traffic_controller_governance_with_kueue">Module 4: The Traffic Controller (Governance with Kueue)</a>
<ul class="sectlevel1">
<li><a href="#_the_pinning_vs_queuing_dilemma">The "Pinning" vs. "Queuing" Dilemma</a></li>
<li><a href="#_step_0_enabling_the_service_day_0_config">Step 0: Enabling the Service (Day 0 Config)</a></li>
<li><a href="#_step_1_define_the_resource_flavor_the_physical_layer">Step 1: Define the Resource Flavor (The Physical Layer)</a></li>
<li><a href="#_step_2_define_the_cluster_queue_the_policy_layer">Step 2: Define the Cluster Queue (The Policy Layer)</a></li>
<li><a href="#_step_3_define_the_local_queue_the_bridge">Step 3: Define the Local Queue (The Bridge)</a></li>
<li><a href="#_step_4_the_hardware_profile_the_user_menu">Step 4: The Hardware Profile (The User Menu)</a></li>
<li><a href="#_summary_the_full_supply_chain">Summary: The Full Supply Chain</a></li>
<li><a href="#_next_steps_4">Next Steps</a></li>
</ul>
</li>
<li><a href="#_module_2b_the_heavy_lifters_multi_gpu_aggregation">Module 2b: The Heavy Lifters (Multi-GPU Aggregation)</a>
<ul class="sectlevel1">
<li><a href="#_concept_the_voltron_strategy">Concept: The "Voltron" Strategy</a></li>
<li><a href="#_step_1_the_basic_bundle_profile">Step 1: The Basic "Bundle" Profile</a></li>
<li><a href="#_step_2_the_topology_trap_critical_warning">Step 2: The Topology Trap (Critical Warning)</a></li>
<li><a href="#_step_3_aggregation_with_kueue_governance">Step 3: Aggregation with Kueue (Governance)</a></li>
<li><a href="#_lab_create_a_heavy_lifter">Lab: Create a "Heavy Lifter"</a></li>
<li><a href="#_summary_the_scaling_up_strategy">Summary: The "Scaling Up" Strategy</a></li>
</ul>
</li>
</ul>
</div>
<div class="sect1">
<h2 id="_phase_1_the_flight_check_validation"><a class="anchor" href="#_phase_1_the_flight_check_validation"></a>Phase 1: The Flight Check (Validation)</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Before you invite users, you must verify the "Supply Chain" from the Node up to the Dashboard.</p>
</div>
<div class="sect2">
<h3 id="_step_1_verify_the_advertised_capacity"><a class="anchor" href="#_step_1_verify_the_advertised_capacity"></a>Step 1: Verify the Advertised Capacity</h3>
<div class="paragraph">
<p>The first truth is the Node status. If the Node doesn&#8217;t see the slices, OpenShift AI never will.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash"># For Time-Slicing Check
oc describe node &lt;node-name&gt; | grep "nvidia.com/gpu"

# For MIG Check
oc describe node &lt;node-name&gt; | grep "nvidia.com/mig"</code></pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Success:</strong> You see a number greater than 1 (e.g., <code>nvidia.com/gpu: 4</code>).</p>
</li>
<li>
<p><strong>Failure:</strong> You see <code>0</code> or <code>1</code> (The configuration didn&#8217;t apply).</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="_step_2_verify_the_dashboard_menu"><a class="anchor" href="#_step_2_verify_the_dashboard_menu"></a>Step 2: Verify the Dashboard "Menu"</h3>
<div class="paragraph">
<p>The second truth is the User Interface.
1. Log in to RHOAI Dashboard.
2. Go to <strong>Settings &#8594; Hardware Profiles</strong>.
3. Confirm your new profiles (e.g., "NVIDIA Shared" or "MIG 1g.5gb") are listed and <strong>Enabled</strong>.</p>
</div>
</div>
<div class="sect2">
<h3 id="_step_3_the_smoke_test_pod_execution"><a class="anchor" href="#_step_3_the_smoke_test_pod_execution"></a>Step 3: The Smoke Test (Pod Execution)</h3>
<div class="paragraph">
<p>The final truth is execution. A node might advertise capacity but fail to launch a container due to driver errors. Run this simple "Hello World" pod to test the slice.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: v1
kind: Pod
metadata:
  name: gpu-smoke-test
spec:
  restartPolicy: OnFailure
  containers:
  - name: cuda-test
    image: nvcr.io/nvidia/k8s/cuda-sample:vectoradd-cuda11.7.1
    resources:
      limits:
        nvidia.com/gpu: 1 # &lt;1&gt; Change this to 'nvidia.com/mig-1g.5gb' for MIG</code></pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Result:</strong> <code>kubectl get pod gpu-smoke-test</code> should show <code>Completed</code>.</p>
</li>
<li>
<p><strong>Logs:</strong> <code>kubectl logs gpu-smoke-test</code> should show <code>Test PASSED</code>.</p>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_phase_2_troubleshooting_time_slicing"><a class="anchor" href="#_phase_2_troubleshooting_time_slicing"></a>Phase 2: Troubleshooting Time-Slicing</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_scenario_a_the_silent_oom"><a class="anchor" href="#_scenario_a_the_silent_oom"></a>Scenario A: The "Silent OOM"</h3>
<div class="ulist">
<ul>
<li>
<p><strong>Symptom:</strong> Users report their kernels are dying randomly, or the "Kernel Restarting" message appears constantly in Jupyter.</p>
</li>
<li>
<p><strong>Root Cause:</strong> Time-slicing shares memory. User A used 10GB, User B used 14GB, and the physical card only has 16GB. The GPU driver kills the process to save itself.</p>
</li>
<li>
<p><strong>Fix:</strong> You cannot enforce memory limits on the GPU hardware with time-slicing. You must enforce them at the <strong>Container Level</strong> (RAM) to discourage massive data loading, or move that user to a MIG profile.</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="_scenario_b_replicas_not_showing_up"><a class="anchor" href="#_scenario_b_replicas_not_showing_up"></a>Scenario B: "Replicas Not Showing Up"</h3>
<div class="ulist">
<ul>
<li>
<p><strong>Symptom:</strong> You applied the ConfigMap, but <code>oc describe node</code> still shows <code>nvidia.com/gpu: 1</code>.</p>
</li>
<li>
<p><strong>Fix:</strong></p>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Check the Operator logs:
<code>oc logs -n nvidia-gpu-operator -l app=gpu-operator-feature-discovery</code></p>
</li>
<li>
<p><strong>Common Error:</strong> The <code>ClusterPolicy</code> was not patched to reference the ConfigMap name.</p>
</li>
<li>
<p><strong>Common Error:</strong> The ConfigMap key (<code>any</code> or <code>tesla-t4</code>) does not match the actual GPU label on the node.</p>
</li>
</ol>
</div>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_phase_3_troubleshooting_mig"><a class="anchor" href="#_phase_3_troubleshooting_mig"></a>Phase 3: Troubleshooting MIG</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_scenario_c_the_stuck_drain"><a class="anchor" href="#_scenario_c_the_stuck_drain"></a>Scenario C: The "Stuck Drain"</h3>
<div class="ulist">
<ul>
<li>
<p><strong>Symptom:</strong> You applied the MIG label, but the node is stuck in <code>SchedulingDisabled</code> or <code>NotReady</code> for hours.</p>
</li>
<li>
<p><strong>Root Cause:</strong> The GPU Operator is trying to drain the node to repartition the hardware, but a Pod (usually a system pod or storage pod) is refusing to leave.</p>
</li>
<li>
<p><strong>Fix:</strong></p>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Check pending evictions: <code>oc get pods --all-namespaces --field-selector spec.nodeName=&lt;node-name&gt;</code></p>
</li>
<li>
<p>Force the drain if necessary (use with caution in prod).</p>
</li>
</ol>
</div>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="_scenario_d_profile_mismatch"><a class="anchor" href="#_scenario_d_profile_mismatch"></a>Scenario D: "Profile Mismatch"</h3>
<div class="ulist">
<ul>
<li>
<p><strong>Symptom:</strong> The user selects the MIG Profile, but the Pod stays in <code>Pending</code>.</p>
</li>
<li>
<p><strong>Error Message:</strong> <code>0/10 nodes are available: 10 Insufficient nvidia.com/mig-1g.5gb.</code></p>
</li>
<li>
<p><strong>Root Cause:</strong> A typo in the string.</p>
</li>
<li>
<p><strong>The Reality Check:</strong></p>
</li>
<li>
<p>NFD advertises: <code>nvidia.com/mig-1g.5gb</code></p>
</li>
<li>
<p>You requested: <code>nvidia.com/mig-1g.5gb</code> (Matches?)</p>
</li>
<li>
<p><strong>Watch out for memory variations:</strong> Sometimes NFD sees <code>mig-1g.10gb</code> on newer A100 models. Always copy-paste from <code>oc describe node</code>.</p>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_summary_of_commands"><a class="anchor" href="#_summary_of_commands"></a>Summary of Commands</h2>
<div class="sectionbody">
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 33.3333%;">
<col style="width: 66.6667%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Goal</th>
<th class="tableblock halign-left valign-top">Command</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Check Capacity</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>oc describe node &lt;node&gt; | grep nvidia</code></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Check Labels</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>oc get node &lt;node&gt; --show-labels</code></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Restart NFD</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>oc delete pod -n nvidia-gpu-operator -l app=nfd-worker</code></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>View Operator Logs</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>oc logs -f deployment/gpu-operator -n nvidia-gpu-operator</code></p></td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="sect1">
<h2 id="_course_completion"><a class="anchor" href="#_course_completion"></a>Course Completion</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Congratulations! You have successfully built, configured, and verified a <strong>GPU-as-a-Service</strong> allocation engine.</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>You learned:</strong> How to stop "GPU Hoarding."</p>
</li>
<li>
<p><strong>You built:</strong> Time-Slicing for density and MIG for isolation.</p>
</li>
<li>
<p><strong>You verified:</strong> That the supply chain from Hardware to Dashboard is intact.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>Now, go optimize those clusters.</strong></p>
</div>
<div class="paragraph">
<p>+======+========</p>
</div>
</div>
</div>
<h1 id="_module_4_the_traffic_controller_governance_with_kueue" class="sect0"><a class="anchor" href="#_module_4_the_traffic_controller_governance_with_kueue"></a>Module 4: The Traffic Controller (Governance with Kueue)</h1>
<div class="paragraph">
<p><strong>How to stop the "Fastest Finger First" race by implementing Fair Share Queuing.</strong></p>
</div>
<div class="quoteblock abstract">
<blockquote>
You have sliced your GPUs (Module 2) and partitioned your hardware (Module 3). But if you simply expose these to users, the first team to log in will consume everything. This module introduces <strong>Kueue</strong>, the traffic controller that manages quotas, priority, and fairness.
</blockquote>
</div>
<div id="toc" class="toc">
<div id="toctitle" class="title">Table of Contents</div>
<ul class="sectlevel1">
<li><a href="#_the_utilization_gap">The Utilization Gap</a></li>
<li><a href="#_the_solution_virtualizing_the_accelerator">The Solution: Virtualizing the Accelerator</a></li>
<li><a href="#_the_architecture_how_it_works">The Architecture: How It Works</a></li>
<li><a href="#_technical_preview_the_clusterpolicy">Technical Preview: The ClusterPolicy</a></li>
<li><a href="#_next_steps">Next Steps</a></li>
<li><a href="#_the_efficiency_engine_time_slicing">The Efficiency Engine (Time-Slicing)</a>
<ul class="sectlevel1">
<li><a href="#_concept_the_gym_membership_model">Concept: The Gym Membership Model</a></li>
<li><a href="#_step_1_define_the_slicing_configuration">Step 1: Define the Slicing Configuration</a></li>
<li><a href="#_step_2_activate_the_configuration_in_clusterpolicy">Step 2: Activate the Configuration in ClusterPolicy</a></li>
<li><a href="#_step_3_verify_the_magic_node_capacity">Step 3: Verify the "Magic" (Node Capacity)</a></li>
<li><a href="#_step_4_the_hardware_profile">Step 4: The Hardware Profile</a></li>
<li><a href="#_lab_try_it_yourself">Lab: Try it Yourself</a></li>
<li><a href="#_next_steps_2">Next Steps</a></li>
</ul>
</li>
<li><a href="#_the_efficiency_engine_time_slicing_2">The Efficiency Engine (Time-Slicing)</a>
<ul class="sectlevel1">
<li><a href="#_concept_the_gym_membership_model_2">Concept: The Gym Membership Model</a></li>
<li><a href="#_step_1_define_the_slicing_configuration_2">Step 1: Define the Slicing Configuration</a></li>
<li><a href="#_step_2_activate_the_configuration_in_clusterpolicy_2">Step 2: Activate the Configuration in ClusterPolicy</a></li>
<li><a href="#_step_3_verify_the_magic_node_capacity_2">Step 3: Verify the "Magic" (Node Capacity)</a></li>
<li><a href="#_step_4_the_hardware_profile_2">Step 4: The Hardware Profile</a></li>
<li><a href="#_lab_try_it_yourself_2">Lab: Try it Yourself</a></li>
<li><a href="#_next_steps_3">Next Steps</a></li>
</ul>
</li>
<li><a href="#_trust_but_verify_validation_troubleshooting">Trust but Verify (Validation &amp; Troubleshooting)</a>
<ul class="sectlevel1">
<li><a href="#_phase_1_the_flight_check_validation">Phase 1: The Flight Check (Validation)</a>
<ul class="sectlevel2">
<li><a href="#_step_1_verify_the_advertised_capacity">Step 1: Verify the Advertised Capacity</a></li>
<li><a href="#_step_2_verify_the_dashboard_menu">Step 2: Verify the Dashboard "Menu"</a></li>
<li><a href="#_step_3_the_smoke_test_pod_execution">Step 3: The Smoke Test (Pod Execution)</a></li>
</ul>
</li>
<li><a href="#_phase_2_troubleshooting_time_slicing">Phase 2: Troubleshooting Time-Slicing</a>
<ul class="sectlevel2">
<li><a href="#_scenario_a_the_silent_oom">Scenario A: The "Silent OOM"</a></li>
<li><a href="#_scenario_b_replicas_not_showing_up">Scenario B: "Replicas Not Showing Up"</a></li>
</ul>
</li>
<li><a href="#_phase_3_troubleshooting_mig">Phase 3: Troubleshooting MIG</a>
<ul class="sectlevel2">
<li><a href="#_scenario_c_the_stuck_drain">Scenario C: The "Stuck Drain"</a></li>
<li><a href="#_scenario_d_profile_mismatch">Scenario D: "Profile Mismatch"</a></li>
</ul>
</li>
<li><a href="#_summary_of_commands">Summary of Commands</a></li>
<li><a href="#_course_completion">Course Completion</a></li>
</ul>
</li>
<li><a href="#_module_4_the_traffic_controller_governance_with_kueue">Module 4: The Traffic Controller (Governance with Kueue)</a>
<ul class="sectlevel1">
<li><a href="#_the_pinning_vs_queuing_dilemma">The "Pinning" vs. "Queuing" Dilemma</a></li>
<li><a href="#_step_0_enabling_the_service_day_0_config">Step 0: Enabling the Service (Day 0 Config)</a></li>
<li><a href="#_step_1_define_the_resource_flavor_the_physical_layer">Step 1: Define the Resource Flavor (The Physical Layer)</a></li>
<li><a href="#_step_2_define_the_cluster_queue_the_policy_layer">Step 2: Define the Cluster Queue (The Policy Layer)</a></li>
<li><a href="#_step_3_define_the_local_queue_the_bridge">Step 3: Define the Local Queue (The Bridge)</a></li>
<li><a href="#_step_4_the_hardware_profile_the_user_menu">Step 4: The Hardware Profile (The User Menu)</a></li>
<li><a href="#_summary_the_full_supply_chain">Summary: The Full Supply Chain</a></li>
<li><a href="#_next_steps_4">Next Steps</a></li>
</ul>
</li>
<li><a href="#_module_2b_the_heavy_lifters_multi_gpu_aggregation">Module 2b: The Heavy Lifters (Multi-GPU Aggregation)</a>
<ul class="sectlevel1">
<li><a href="#_concept_the_voltron_strategy">Concept: The "Voltron" Strategy</a></li>
<li><a href="#_step_1_the_basic_bundle_profile">Step 1: The Basic "Bundle" Profile</a></li>
<li><a href="#_step_2_the_topology_trap_critical_warning">Step 2: The Topology Trap (Critical Warning)</a></li>
<li><a href="#_step_3_aggregation_with_kueue_governance">Step 3: Aggregation with Kueue (Governance)</a></li>
<li><a href="#_lab_create_a_heavy_lifter">Lab: Create a "Heavy Lifter"</a></li>
<li><a href="#_summary_the_scaling_up_strategy">Summary: The "Scaling Up" Strategy</a></li>
</ul>
</li>
</ul>
</div>
<div class="sect1">
<h2 id="_the_pinning_vs_queuing_dilemma"><a class="anchor" href="#_the_pinning_vs_queuing_dilemma"></a>The "Pinning" vs. "Queuing" Dilemma</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Before you build your final Hardware Profile, you must make a critical architectural decision.</p>
</div>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 50%;">
<col style="width: 50%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Strategy A: Static Pinning (The Old Way)</th>
<th class="tableblock halign-left valign-top">Strategy B: Dynamic Queuing (The New Way)</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Mechanism:</strong> The Hardware Profile contains <code>nodeSelectors</code> and <code>tolerations</code> directly.</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Mechanism:</strong> The Hardware Profile contains a <code>LocalQueue</code> reference. The placement logic moves to a <strong>ResourceFlavor</strong>.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Behavior:</strong> "I want this specific node." If full, the pod stays Pending forever.</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Behavior:</strong> "I want a resource like this." If full, the job is queued and prioritized.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Fairness:</strong> None. First come, first served.</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Fairness:</strong> "Fair Share." Team A can borrow Team B&#8217;s unused quota.</p></td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="sect1">
<h2 id="_step_0_enabling_the_service_day_0_config"><a class="anchor" href="#_step_0_enabling_the_service_day_0_config"></a>Step 0: Enabling the Service (Day 0 Config)</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Kueue is installed but "dormant" by default. You must flip three switches to make the queuing features visible in the RHOAI Dashboard.</p>
</div>
<div class="paragraph">
<p><strong>1. Configure the Cluster State</strong>
Ensure your <code>DataScienceCluster</code> resource has the Kueue component set to <code>Managed</code> or <code>Unmanaged</code> (if you need custom tuning).</p>
</div>
<div class="paragraph">
<p><strong>2. Enable the Dashboard UI</strong>
You must tell the RHOAI Dashboard to show the "Queue" options in the menu.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: opendatahub.io/v1alpha
kind: OdhDashboardConfig
metadata:
  name: odh-dashboard-config
  namespace: redhat-ods-applications
spec:
  # Enable the Queue selection UI
  disableKueue: false</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong>3. Label the Namespace</strong>
Kueue only watches projects that are explicitly invited. You must label the user&#8217;s namespace.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">oc label namespace &lt;user-project&gt; kueue.openshift.io/managed=true</code></pre>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_step_1_define_the_resource_flavor_the_physical_layer"><a class="anchor" href="#_step_1_define_the_resource_flavor_the_physical_layer"></a>Step 1: Define the Resource Flavor (The Physical Layer)</h2>
<div class="sectionbody">
<div class="paragraph">
<p>In the Queuing model, the Hardware Profile becomes "dumb." It doesn&#8217;t know about Taints or MIG labels. Instead, we define a <strong>ResourceFlavor</strong> object that holds the physical truth.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: kueue.x-k8s.io/v1beta1
kind: ResourceFlavor
metadata:
  name: flavor-nvidia-mig-small
spec:
  nodeLabels:
    nvidia.com/mig-1g.5gb: "true"  # &lt;1&gt; The NFD Label
  tolerations:
  - key: "nvidia.com/gpu"          # &lt;2&gt; The Taint Key
    operator: "Exists"
    effect: "NoSchedule"</code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td><strong>The Connection:</strong> This connects the abstract flavor to the specific MIG slice you created in Module 3.</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td><strong>The Key:</strong> This grants access to the Tainted node.</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_step_2_define_the_cluster_queue_the_policy_layer"><a class="anchor" href="#_step_2_define_the_cluster_queue_the_policy_layer"></a>Step 2: Define the Cluster Queue (The Policy Layer)</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Now, we define the rules. Who gets what?</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: kueue.x-k8s.io/v1beta1
kind: ClusterQueue
metadata:
  name: cluster-queue-gpu-pool
spec:
  namespaceSelector: {} # Applies to all projects
  resourceGroups:
  - coveredResources: ["nvidia.com/mig-1g.5gb"]
    flavors:
    - name: flavor-nvidia-mig-small
      resources:
      - name: "nvidia.com/mig-1g.5gb"
        nominalQuota: 10  <i class="conum" data-value="1"></i><b>(1)</b>
        borrowingLimit: 5 <i class="conum" data-value="2"></i><b>(2)</b></code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td><strong>Guaranteed Quota:</strong> The team is promised 10 slices.</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td><strong>Elasticity:</strong> If the cluster is empty, they can borrow 5 more (Total 15).</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_step_3_define_the_local_queue_the_bridge"><a class="anchor" href="#_step_3_define_the_local_queue_the_bridge"></a>Step 3: Define the Local Queue (The Bridge)</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Users cannot see the <code>ClusterQueue</code>. You must create a <code>LocalQueue</code> inside their project that acts as a connector.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: kueue.x-k8s.io/v1beta1
kind: LocalQueue
metadata:
  namespace: &lt;user-project&gt;
  name: local-queue-gpu
spec:
  clusterQueue: cluster-queue-gpu-pool # Reference to Step 2</code></pre>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_step_4_the_hardware_profile_the_user_menu"><a class="anchor" href="#_step_4_the_hardware_profile_the_user_menu"></a>Step 4: The Hardware Profile (The User Menu)</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Finally, create the profile. When using Kueue, your Hardware Profile <strong>must not</strong> contain selectors.</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Incorrect (Legacy):</strong> Profile contains <code>tolerations: [{key: nvidia&#8230;&#8203;}]</code></p>
</li>
<li>
<p><strong>Correct (Kueue):</strong> Profile contains <strong>only</strong> the queue label.</p>
</li>
</ul>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: dashboard.opendatahub.io/v1alpha1
kind: HardwareProfile
metadata:
  name: profile-mig-small-queue
  namespace: redhat-ods-applications
spec:
  displayName: "Small GPU Slice (Managed)"
  description: "Queued access to 1g.5gb slices."
  identifiers:
    - identifier: nvidia.com/mig-1g.5gb
      count: 1
  # CRITICAL: No nodeSelector or tolerations here!
  # The queuing system handles placement.</code></pre>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_summary_the_full_supply_chain"><a class="anchor" href="#_summary_the_full_supply_chain"></a>Summary: The Full Supply Chain</h2>
<div class="sectionbody">
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong>Physical:</strong> You labeled the node <code>mig-1g.5gb</code> (Module 3).</p>
</li>
<li>
<p><strong>Logical:</strong> You mapped that label to a <code>ResourceFlavor</code> (Module 4).</p>
</li>
<li>
<p><strong>Policy:</strong> You assigned that flavor a Quota in the <code>ClusterQueue</code>.</p>
</li>
<li>
<p><strong>User:</strong> The User selects the Profile, which submits a ticket to the Queue.</p>
</li>
</ol>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_next_steps_4"><a class="anchor" href="#_next_steps_4"></a>Next Steps</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Now that your governance layer is in place, proceed to <strong>Module 5: Validation &amp; Troubleshooting</strong> to verify the end-to-end flow.</p>
</div>
<div class="paragraph">
<p>===============</p>
</div>
</div>
</div>
<h1 id="_module_2b_the_heavy_lifters_multi_gpu_aggregation" class="sect0"><a class="anchor" href="#_module_2b_the_heavy_lifters_multi_gpu_aggregation"></a>Module 2b: The Heavy Lifters (Multi-GPU Aggregation)</h1>
<div class="paragraph">
<p><strong>How to bundle smaller cards (T4, L40S) to rival the power of an H100.</strong></p>
</div>
<div class="quoteblock abstract">
<blockquote>
Not every organization can afford a fleet of H100s. Often, you will find yourself with nodes full of smaller cards like NVIDIA T4s or L40S. This module teaches you the "Aggregation Strategy"—how to bundle multiple physical GPUs into a single Hardware Profile to power Large Language Model (LLM) fine-tuning and distributed training.
</blockquote>
</div>
<div id="toc" class="toc">
<div id="toctitle" class="title">Table of Contents</div>
<ul class="sectlevel1">
<li><a href="#_the_utilization_gap">The Utilization Gap</a></li>
<li><a href="#_the_solution_virtualizing_the_accelerator">The Solution: Virtualizing the Accelerator</a></li>
<li><a href="#_the_architecture_how_it_works">The Architecture: How It Works</a></li>
<li><a href="#_technical_preview_the_clusterpolicy">Technical Preview: The ClusterPolicy</a></li>
<li><a href="#_next_steps">Next Steps</a></li>
<li><a href="#_the_efficiency_engine_time_slicing">The Efficiency Engine (Time-Slicing)</a>
<ul class="sectlevel1">
<li><a href="#_concept_the_gym_membership_model">Concept: The Gym Membership Model</a></li>
<li><a href="#_step_1_define_the_slicing_configuration">Step 1: Define the Slicing Configuration</a></li>
<li><a href="#_step_2_activate_the_configuration_in_clusterpolicy">Step 2: Activate the Configuration in ClusterPolicy</a></li>
<li><a href="#_step_3_verify_the_magic_node_capacity">Step 3: Verify the "Magic" (Node Capacity)</a></li>
<li><a href="#_step_4_the_hardware_profile">Step 4: The Hardware Profile</a></li>
<li><a href="#_lab_try_it_yourself">Lab: Try it Yourself</a></li>
<li><a href="#_next_steps_2">Next Steps</a></li>
</ul>
</li>
<li><a href="#_the_efficiency_engine_time_slicing_2">The Efficiency Engine (Time-Slicing)</a>
<ul class="sectlevel1">
<li><a href="#_concept_the_gym_membership_model_2">Concept: The Gym Membership Model</a></li>
<li><a href="#_step_1_define_the_slicing_configuration_2">Step 1: Define the Slicing Configuration</a></li>
<li><a href="#_step_2_activate_the_configuration_in_clusterpolicy_2">Step 2: Activate the Configuration in ClusterPolicy</a></li>
<li><a href="#_step_3_verify_the_magic_node_capacity_2">Step 3: Verify the "Magic" (Node Capacity)</a></li>
<li><a href="#_step_4_the_hardware_profile_2">Step 4: The Hardware Profile</a></li>
<li><a href="#_lab_try_it_yourself_2">Lab: Try it Yourself</a></li>
<li><a href="#_next_steps_3">Next Steps</a></li>
</ul>
</li>
<li><a href="#_trust_but_verify_validation_troubleshooting">Trust but Verify (Validation &amp; Troubleshooting)</a>
<ul class="sectlevel1">
<li><a href="#_phase_1_the_flight_check_validation">Phase 1: The Flight Check (Validation)</a>
<ul class="sectlevel2">
<li><a href="#_step_1_verify_the_advertised_capacity">Step 1: Verify the Advertised Capacity</a></li>
<li><a href="#_step_2_verify_the_dashboard_menu">Step 2: Verify the Dashboard "Menu"</a></li>
<li><a href="#_step_3_the_smoke_test_pod_execution">Step 3: The Smoke Test (Pod Execution)</a></li>
</ul>
</li>
<li><a href="#_phase_2_troubleshooting_time_slicing">Phase 2: Troubleshooting Time-Slicing</a>
<ul class="sectlevel2">
<li><a href="#_scenario_a_the_silent_oom">Scenario A: The "Silent OOM"</a></li>
<li><a href="#_scenario_b_replicas_not_showing_up">Scenario B: "Replicas Not Showing Up"</a></li>
</ul>
</li>
<li><a href="#_phase_3_troubleshooting_mig">Phase 3: Troubleshooting MIG</a>
<ul class="sectlevel2">
<li><a href="#_scenario_c_the_stuck_drain">Scenario C: The "Stuck Drain"</a></li>
<li><a href="#_scenario_d_profile_mismatch">Scenario D: "Profile Mismatch"</a></li>
</ul>
</li>
<li><a href="#_summary_of_commands">Summary of Commands</a></li>
<li><a href="#_course_completion">Course Completion</a></li>
</ul>
</li>
<li><a href="#_module_4_the_traffic_controller_governance_with_kueue">Module 4: The Traffic Controller (Governance with Kueue)</a>
<ul class="sectlevel1">
<li><a href="#_the_pinning_vs_queuing_dilemma">The "Pinning" vs. "Queuing" Dilemma</a></li>
<li><a href="#_step_0_enabling_the_service_day_0_config">Step 0: Enabling the Service (Day 0 Config)</a></li>
<li><a href="#_step_1_define_the_resource_flavor_the_physical_layer">Step 1: Define the Resource Flavor (The Physical Layer)</a></li>
<li><a href="#_step_2_define_the_cluster_queue_the_policy_layer">Step 2: Define the Cluster Queue (The Policy Layer)</a></li>
<li><a href="#_step_3_define_the_local_queue_the_bridge">Step 3: Define the Local Queue (The Bridge)</a></li>
<li><a href="#_step_4_the_hardware_profile_the_user_menu">Step 4: The Hardware Profile (The User Menu)</a></li>
<li><a href="#_summary_the_full_supply_chain">Summary: The Full Supply Chain</a></li>
<li><a href="#_next_steps_4">Next Steps</a></li>
</ul>
</li>
<li><a href="#_module_2b_the_heavy_lifters_multi_gpu_aggregation">Module 2b: The Heavy Lifters (Multi-GPU Aggregation)</a>
<ul class="sectlevel1">
<li><a href="#_concept_the_voltron_strategy">Concept: The "Voltron" Strategy</a></li>
<li><a href="#_step_1_the_basic_bundle_profile">Step 1: The Basic "Bundle" Profile</a></li>
<li><a href="#_step_2_the_topology_trap_critical_warning">Step 2: The Topology Trap (Critical Warning)</a></li>
<li><a href="#_step_3_aggregation_with_kueue_governance">Step 3: Aggregation with Kueue (Governance)</a></li>
<li><a href="#_lab_create_a_heavy_lifter">Lab: Create a "Heavy Lifter"</a></li>
<li><a href="#_summary_the_scaling_up_strategy">Summary: The "Scaling Up" Strategy</a></li>
</ul>
</li>
</ul>
</div>
<div class="sect1">
<h2 id="_concept_the_voltron_strategy"><a class="anchor" href="#_concept_the_voltron_strategy"></a>Concept: The "Voltron" Strategy</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Slicing (Module 2) and MIG (Module 3) are about taking a big resource and breaking it down. Aggregation is the opposite: taking small resources and combining them.</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>The Scenario:</strong> A Data Scientist needs 64GB of VRAM to fine-tune Llama-3-70B.</p>
</li>
<li>
<p><strong>The Problem:</strong> You only have NVIDIA L40S cards (48GB VRAM each). A single card will crash with an OOM (Out of Memory) error.</p>
</li>
<li>
<p><strong>The Solution:</strong> You create a Hardware Profile that binds <strong>2x L40S cards</strong> together. The workload sees 96GB of total addressable VRAM (if using distributed libraries like Ray or PyTorch DDP).</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_step_1_the_basic_bundle_profile"><a class="anchor" href="#_step_1_the_basic_bundle_profile"></a>Step 1: The Basic "Bundle" Profile</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The core mechanism for aggregation is simple: you ask for more than one.</p>
</div>
<div class="paragraph">
<p>In your Hardware Profile, you change the <code>count</code> parameter.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: dashboard.opendatahub.io/v1alpha1
kind: HardwareProfile
metadata:
  name: nvidia-dual-l40s
  namespace: redhat-ods-applications
spec:
  displayName: "Dual L40S Station (96GB VRAM)"
  description: "Bundles 2 physical GPUs for larger model training."
  identifiers:
    - identifier: nvidia.com/gpu
      count: 2  # &lt;1&gt; The Bundle Request
  tolerations:
    - key: "nvidia.com/gpu"
      operator: "Exists"
      effect: "NoSchedule"</code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td><strong>The Scheduler&#8217;s Job:</strong> Kubernetes will strictly look for a node that has <strong>at least</strong> 2 free GPUs. If a node only has 1 free, it will be skipped.</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_step_2_the_topology_trap_critical_warning"><a class="anchor" href="#_step_2_the_topology_trap_critical_warning"></a>Step 2: The Topology Trap (Critical Warning)</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Asking for "2 GPUs" is dangerous if you don&#8217;t care <strong>which</strong> 2 you get.</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>The Risk:</strong> In some bare-metal servers, GPU 0 and GPU 1 might be on different PCIe switches (NUMA nodes). Communication between them is slow.</p>
</li>
<li>
<p><strong>The Requirement:</strong> For training, you need GPUs connected via <strong>NVLink</strong> or high-speed interconnects.</p>
</li>
<li>
<p><strong>The Fix:</strong> You must use <code>nodeSelectors</code> to pin this profile to machine types known to have good topology.</p>
</li>
</ul>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">  # Inside the Hardware Profile spec
  nodeSelector:
    # Example: Pinning to a specific Dell or AWS machine type
    node.kubernetes.io/instance-type: "p5.48xlarge"
    # OR using a custom label you applied to your high-performance racks
    hardware.topology/interconnect: "nvlink"</code></pre>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_step_3_aggregation_with_kueue_governance"><a class="anchor" href="#_step_3_aggregation_with_kueue_governance"></a>Step 3: Aggregation with Kueue (Governance)</h2>
<div class="sectionbody">
<div class="paragraph">
<p>If you are using the Governance architecture (from Module 4), you do not put the <code>count</code> in the Hardware Profile. You put it in the <strong>ResourceFlavor</strong>.</p>
</div>
<div class="paragraph">
<p>This allows you to treat "Dual GPU" as a distinct class of service with its own quota.</p>
</div>
<div class="paragraph">
<p><strong>1. Define the Flavor (The Physical Bundle)</strong></p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: kueue.x-k8s.io/v1beta1
kind: ResourceFlavor
metadata:
  name: flavor-dual-gpu
spec:
  nodeLabels:
    nvidia.com/gpu.count: "2" # &lt;1&gt; Target nodes with 2 cards
  tolerations:
  - key: "nvidia.com/gpu"
    operator: "Exists"</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong>2. Define the Profile (The Logical Pointer)</strong></p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: dashboard.opendatahub.io/v1alpha1
kind: HardwareProfile
metadata:
  name: profile-dual-gpu-queue
spec:
  displayName: "Dual GPU Training (Queued)"
  identifiers:
    - identifier: nvidia.com/gpu
      count: 2 <i class="conum" data-value="1"></i><b>(1)</b></code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td><strong>Note:</strong> Even with Kueue, passing <code>count: 2</code> here helps the dashboard validate the request, but the heavy lifting of admission checks is done by the ClusterQueue limits.</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_lab_create_a_heavy_lifter"><a class="anchor" href="#_lab_create_a_heavy_lifter"></a>Lab: Create a "Heavy Lifter"</h2>
<div class="sectionbody">
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong>Identify Multi-GPU Nodes:</strong> Run <code>oc get nodes -L nvidia.com/gpu.count</code> to find nodes with &gt;1 GPU.</p>
</li>
<li>
<p><strong>Create the Profile:</strong> Define a <code>HardwareProfile</code> requesting <code>count: 2</code>.</p>
</li>
<li>
<p><strong>Launch a Workbench:</strong> Select the profile.</p>
</li>
<li>
<p><strong>Verify inside the Pod:</strong> Open a terminal in Jupyter and run:
<code>nvidia-smi</code>
<strong>Result:</strong> You should see two distinct GPU devices listed (Device 0 and Device 1).</p>
</li>
</ol>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_summary_the_scaling_up_strategy"><a class="anchor" href="#_summary_the_scaling_up_strategy"></a>Summary: The "Scaling Up" Strategy</h2>
<div class="sectionbody">
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 50%;">
<col style="width: 50%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Strategy</th>
<th class="tableblock halign-left valign-top">Scaling Down (Slicing/MIG)</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Scaling Up (Aggregation)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Goal</strong></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Efficiency (ROI)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Power (Performance)</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Mechanism</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">1 GPU &#8594; Many Users</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Many GPUs &#8594; 1 User</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Key Param</strong></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>identifiers</code> (mig-1g.5gb)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>count</code> (2, 4, 8)</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Ideal For</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Notebooks, Dev, Inference</p></td>
</tr>
</tbody>
</table>
</div>
</div>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <img src="../../../_/img/rhl-logo-red.png" height="40px" alt="Red Hat"  href="https://redhat.com" >
</footer><script id="site-script" src="../../../_/js/site.js" data-ui-root-path="../../../_"></script>
<script async src="../../../_/js/vendor/highlight.js"></script>
  </body>
</html>
