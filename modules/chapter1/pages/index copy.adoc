= GPU-as-a-Service: Optimizing ROI with Slicing & MIG
:navtitle: GPU Optimization Intro
:imagesdir: ../images
:toc: macro

*Unlock the hidden capacity in your data center. Turn one physical GPU into seven virtual accelerators.*

[abstract]
You have spent the budget on NVIDIA A100s or H100s, but your monitoring dashboards tell a painful story: **Low Utilization.** This module teaches you how to architect a "GPU-as-a-Service" model using Red Hat OpenShift AI (RHOAI), the NVIDIA GPU Operator, and Node Feature Discovery (NFD).

toc::[]

== The Utilization Gap

In a standard Kubernetes environment, a GPU is a "monolithic" resource. If a data scientist requests a GPU for a Jupyter notebook to write code, Kubernetes assigns them the _entire_ physical card.

* **The Cost:** That user might only use 2GB of VRAM and 5% of the compute cores.
* **The Waste:** The remaining 75GB of memory and 95% of compute power sits idle, locked away from other users.
* **The Result:** You run out of allocatable GPUs while your actual hardware sits empty.



== The Solution: Virtualizing the Accelerator

To solve this, we move from "Physical Allocation" to "Virtual Allocation." RHOAI supports two primary methods for slicing GPUs, allowing you to squeeze 4x to 7x more users onto the same hardware.

[cols="1,1", options="header"]
|===
|Strategy A: Time-Slicing (The "Soft" Cut)
|Strategy B: MIG (The "Hard" Cut)

|**What it is:** Software-level oversubscription. The GPU context-switches between processes very quickly.
|**What it is:** Hardware-level partitioning. The GPU is electrically divided into distinct instances.

|**Isolation:** Shared Memory (Low Isolation). One user's OOM error can crash the GPU.
|**Isolation:** Dedicated Memory (High Isolation). Faults are contained within the slice.

|**Best For:** Development, Classrooms, Notebooks, Code Generation.
|**Best For:** Production Inference, Stable Training, Multi-tenant SaaS.
|===

== The Architecture: How It Works

This course will guide you through the three layers required to build this engine:

1.  **The Slicer (NVIDIA GPU Operator):**
    You will configure the `ClusterPolicy` to tell the driver how to present the hardware (e.g., "Split this A100 into 7 slices").

2.  **The Advertiser (Node Feature Discovery - NFD):**
    Once sliced, NFD automatically detects the new virtual resources and labels the nodes.
    * *Before:* `nvidia.com/gpu: 1`
    * *After:* `nvidia.com/mig-1g.5gb: 7`

3.  **The Vending Machine (Hardware Profiles):**
    You will create RHOAI Hardware Profiles that target these specific NFD labels, allowing users to select "Small Slice" from a dropdown menu.

== Technical Preview: The ClusterPolicy

You asked: *How do we actually turn this on?*

In the upcoming "Hands-On" modules, we will be editing the **ClusterPolicy** Custom Resource. Here is a preview of the configuration we will apply to enable these features.

[source,yaml]
----
apiVersion: nvidia.com/v1
kind: ClusterPolicy
metadata:
  name: gpu-cluster-policy
spec:
  # 1. ENABLING MIG (The "Hard" Cut)
  # 'mixed' allows a node to run different sized slices simultaneously
  migStrategy: mixed

  # 2. ENABLING TIME-SLICING (The "Soft" Cut)
  # We reference a ConfigMap that defines how many replicas to create
  devicePlugin:
    config:
      name: time-slicing-config
      default: available  # Applies to GPUs not covered by specific configs
----

[IMPORTANT]
.Prerequisites
====
To complete the labs in this course, you will need:

* **Cluster Admin access** to an OpenShift AI 3.x cluster.
* **NVIDIA GPUs** (Pascal architecture or newer for Time-Slicing; Ampere A100/H100 for MIG).
* The **OpenShift CLI (`oc`)**.
====

== Next Steps

Ready to stop the waste?

* Proceed to **Module 2: Configuring Time-Slicing** for the quickest win.
* Jump to **Module 3: Configuring MIG** for advanced isolation.



++=========_++++

= The Efficiency Engine (Time-Slicing)
:navtitle: Configuring Time-Slicing
:imagesdir: ../images
:toc: macro

*How to oversubscribe your hardware and solve the "No GPUs Available" crisis.*

[abstract]
Time-slicing is the quickest way to increase density in your cluster. By enabling software-level interleaving, you can allow multiple workloads to share a single GPU. This module guides you through the three-step configuration process: The Map, The Policy, and The Label.

toc::[]

== Concept: The Gym Membership Model
Think of Time-Slicing like a gym. You might have 100 members but only 10 treadmills. It works because not everyone runs at the exact same second.

* **Without Time-Slicing:** When a user opens a Jupyter Notebook, they "lock the door" to the gym. No one else can enter, even if the user is just reading documentation.
* **With Time-Slicing:** The door is unlocked. Multiple users can enter. The GPU (the treadmill) rapidly switches between them.

[IMPORTANT]
.The Safety Warning
====
**Shared Memory:** Time-slicing provides *compute* isolation (via context switching) but **not memory isolation**. All users share the same VRAM pool.
+
 * *Risk:* If User A runs a massive job that consumes 100% of the VRAM, User B's process will crash (OOM).
+
 * *Mitigation:* Use this strategy for Development, Notebooks, and Education—not for heavy production training.
====

== Step 1: Define the Slicing Configuration
First, we tell the NVIDIA GPU Operator *how* to slice the cards. We do this with a `ConfigMap`.

[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: time-slicing-config
  namespace: nvidia-gpu-operator
data:
  any: |-  # <1>
    version: v1
    sharing:
      timeSlicing:
        resources:
        - name: nvidia.com/gpu
          replicas: 4  # <2>
----
<1> **The Config Key:** We use `any` to apply this generic rule to any NVIDIA GPU. You can also use specific product names (e.g., `tesla-t4`, `a100`) to create different rules for different cards.
<2> **Replicas:** This is the magic number. We are telling the driver to advertise 4 "virtual" GPUs for every 1 physical GPU.

== Step 2: Activate the Configuration in ClusterPolicy
Merely creating the ConfigMap does nothing. We must instruct the GPU Operator to read it.

We patch the `ClusterPolicy` to reference our new map.

[source,yaml]
----
apiVersion: nvidia.com/v1
kind: ClusterPolicy
metadata:
  name: gpu-cluster-policy
spec:
  migStrategy: none  # <1>
  devicePlugin:
    config:
      name: time-slicing-config  # <2>
      default: any  # <3>
----
<1> **Disable MIG:** Time-slicing and MIG cannot usually coexist on the same node. We ensure MIG is off.
<2> **Reference:** Points to the ConfigMap we created in Step 1.
<3> **Default Strategy:** Tells the operator to apply the configuration keyed as `any` (from Step 1) to all GPUs unless specified otherwise.

== Step 3: Verify the "Magic" (Node Capacity)
Once the Operator processes the change, the Device Plugin will restart. You can verify the success by looking at the Node's capacity.

Run the following command:
[source,bash]
----
oc describe node <your-gpu-node> | grep "nvidia.com/gpu"
----

.Output Comparison
[cols="1,1", options="header"]
|===
|Before |After
|Capacity: `nvidia.com/gpu: 1` |Capacity: `nvidia.com/gpu: 4`
|Allocatable: `nvidia.com/gpu: 1` |Allocatable: `nvidia.com/gpu: 4`
|===



The node now claims to have 4 GPUs. Kubernetes doesn't know they are fake; it just schedules pods until the counter hits 0.

== Step 4: The Hardware Profile
Now that the capacity exists, how do users consume it?

With Time-Slicing, the resource name remains `nvidia.com/gpu`. Therefore, you do **not** need a specialized identifier in your Hardware Profile. You simply create a profile for "Shared GPU".

[source,yaml]
----
apiVersion: dashboard.opendatahub.io/v1alpha1
kind: HardwareProfile
metadata:
  name: nvidia-gpu-shared
  namespace: redhat-ods-applications # Global profile
spec:
  displayName: "NVIDIA GPU (Shared)"
  description: "Best for interactive workbooks and coding. Memory is shared with other users."
  identifiers:
    - identifier: nvidia.com/gpu
      count: 1  # <1>
  tolerations:
    - key: "nvidia.com/gpu"
      operator: "Exists"
      effect: "NoSchedule"
----
<1> **The User Request:** The user still asks for "1 GPU". But because the node now has 4 to give, 4 users can successfully launch this profile on the same node.

== Lab: Try it Yourself
1. Create the `time-slicing-config` ConfigMap.
2. Edit the `gpu-cluster-policy` to reference it.
3. Watch the pods in `nvidia-gpu-operator` namespace restart.
4. Verify `oc describe node` shows increased capacity.
5. Create two Notebooks using the "Shared" profile and verify they land on the same node.

== Next Steps
Time-slicing is great for density, but what if you need **guaranteed performance** and **strict isolation**?
Proceed to **Module 3: Configuring MIG** to learn the "Hard" partition strategy.


===+++++======++++=====

= The Efficiency Engine (Time-Slicing)
:navtitle: Configuring Time-Slicing
:imagesdir: ../images
:toc: macro

*How to oversubscribe your hardware and solve the "No GPUs Available" crisis.*

[abstract]
Time-slicing is the quickest way to increase density in your cluster. By enabling software-level interleaving, you can allow multiple workloads to share a single GPU. This module guides you through the three-step configuration process: The Map, The Policy, and The Label.

toc::[]

== Concept: The Gym Membership Model
Think of Time-Slicing like a gym. You might have 100 members but only 10 treadmills. It works because not everyone runs at the exact same second.

* **Without Time-Slicing:** When a user opens a Jupyter Notebook, they "lock the door" to the gym. No one else can enter, even if the user is just reading documentation.
* **With Time-Slicing:** The door is unlocked. Multiple users can enter. The GPU (the treadmill) rapidly switches between them.

[IMPORTANT]
.The Safety Warning
====
**Shared Memory:** Time-slicing provides *compute* isolation (via context switching) but **not memory isolation**. All users share the same VRAM pool.
+
 * *Risk:* If User A runs a massive job that consumes 100% of the VRAM, User B's process will crash (OOM).
+
 * *Mitigation:* Use this strategy for Development, Notebooks, and Education—not for heavy production training.
====

== Step 1: Define the Slicing Configuration
First, we tell the NVIDIA GPU Operator *how* to slice the cards. We do this with a `ConfigMap`.

[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: time-slicing-config
  namespace: nvidia-gpu-operator
data:
  any: |-  # <1>
    version: v1
    sharing:
      timeSlicing:
        resources:
        - name: nvidia.com/gpu
          replicas: 4  # <2>
----
<1> **The Config Key:** We use `any` to apply this generic rule to any NVIDIA GPU. You can also use specific product names (e.g., `tesla-t4`, `a100`) to create different rules for different cards.
<2> **Replicas:** This is the magic number. We are telling the driver to advertise 4 "virtual" GPUs for every 1 physical GPU.

== Step 2: Activate the Configuration in ClusterPolicy
Merely creating the ConfigMap does nothing. We must instruct the GPU Operator to read it.

We patch the `ClusterPolicy` to reference our new map.

[source,yaml]
----
apiVersion: nvidia.com/v1
kind: ClusterPolicy
metadata:
  name: gpu-cluster-policy
spec:
  migStrategy: none  # <1>
  devicePlugin:
    config:
      name: time-slicing-config  # <2>
      default: any  # <3>
----
<1> **Disable MIG:** Time-slicing and MIG cannot usually coexist on the same node. We ensure MIG is off.
<2> **Reference:** Points to the ConfigMap we created in Step 1.
<3> **Default Strategy:** Tells the operator to apply the configuration keyed as `any` (from Step 1) to all GPUs unless specified otherwise.

== Step 3: Verify the "Magic" (Node Capacity)
Once the Operator processes the change, the Device Plugin will restart. You can verify the success by looking at the Node's capacity.

Run the following command:
[source,bash]
----
oc describe node <your-gpu-node> | grep "nvidia.com/gpu"
----

.Output Comparison
[cols="1,1", options="header"]
|===
|Before |After
|Capacity: `nvidia.com/gpu: 1` |Capacity: `nvidia.com/gpu: 4`
|Allocatable: `nvidia.com/gpu: 1` |Allocatable: `nvidia.com/gpu: 4`
|===



The node now claims to have 4 GPUs. Kubernetes doesn't know they are fake; it just schedules pods until the counter hits 0.

== Step 4: The Hardware Profile
Now that the capacity exists, how do users consume it?

With Time-Slicing, the resource name remains `nvidia.com/gpu`. Therefore, you do **not** need a specialized identifier in your Hardware Profile. You simply create a profile for "Shared GPU".

[source,yaml]
----
apiVersion: dashboard.opendatahub.io/v1alpha1
kind: HardwareProfile
metadata:
  name: nvidia-gpu-shared
  namespace: redhat-ods-applications # Global profile
spec:
  displayName: "NVIDIA GPU (Shared)"
  description: "Best for interactive workbooks and coding. Memory is shared with other users."
  identifiers:
    - identifier: nvidia.com/gpu
      count: 1  # <1>
  tolerations:
    - key: "nvidia.com/gpu"
      operator: "Exists"
      effect: "NoSchedule"
----
<1> **The User Request:** The user still asks for "1 GPU". But because the node now has 4 to give, 4 users can successfully launch this profile on the same node.

== Lab: Try it Yourself
1. Create the `time-slicing-config` ConfigMap.
2. Edit the `gpu-cluster-policy` to reference it.
3. Watch the pods in `nvidia-gpu-operator` namespace restart.
4. Verify `oc describe node` shows increased capacity.
5. Create two Notebooks using the "Shared" profile and verify they land on the same node.

== Next Steps
Time-slicing is great for density, but what if you need **guaranteed performance** and **strict isolation**?
Proceed to **Module 3: Configuring MIG** to learn the "Hard" partition strategy.


====---++++++---======


= Trust but Verify (Validation & Troubleshooting)
:navtitle: Validation & Troubleshooting
:imagesdir: ../images
:toc: macro

*How to confirm your allocation engine is running and fix common "GPU Starvation" errors.*

[abstract]
You have applied the ConfigMaps and labeled the nodes. But is the system actually serving slices? This module provides the "Flight Check" procedures to validate your GPU-as-a-Service implementation and troubleshoot the two most common failure modes: The Silent OOM (Time-Slicing) and The Stuck Drain (MIG).

toc::[]

== Phase 1: The Flight Check (Validation)

Before you invite users, you must verify the "Supply Chain" from the Node up to the Dashboard.

=== Step 1: Verify the Advertised Capacity
The first truth is the Node status. If the Node doesn't see the slices, OpenShift AI never will.

[source,bash]
----
# For Time-Slicing Check
oc describe node <node-name> | grep "nvidia.com/gpu"

# For MIG Check
oc describe node <node-name> | grep "nvidia.com/mig"
----

* **Success:** You see a number greater than 1 (e.g., `nvidia.com/gpu: 4`).
* **Failure:** You see `0` or `1` (The configuration didn't apply).

=== Step 2: Verify the Dashboard "Menu"
The second truth is the User Interface.
1. Log in to RHOAI Dashboard.
2. Go to **Settings -> Hardware Profiles**.
3. Confirm your new profiles (e.g., "NVIDIA Shared" or "MIG 1g.5gb") are listed and **Enabled**.



=== Step 3: The Smoke Test (Pod Execution)
The final truth is execution. A node might advertise capacity but fail to launch a container due to driver errors. Run this simple "Hello World" pod to test the slice.

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: gpu-smoke-test
spec:
  restartPolicy: OnFailure
  containers:
  - name: cuda-test
    image: nvcr.io/nvidia/k8s/cuda-sample:vectoradd-cuda11.7.1
    resources:
      limits:
        nvidia.com/gpu: 1 # <1> Change this to 'nvidia.com/mig-1g.5gb' for MIG
----
* **Result:** `kubectl get pod gpu-smoke-test` should show `Completed`.
* **Logs:** `kubectl logs gpu-smoke-test` should show `Test PASSED`.

== Phase 2: Troubleshooting Time-Slicing

=== Scenario A: The "Silent OOM"
* **Symptom:** Users report their kernels are dying randomly, or the "Kernel Restarting" message appears constantly in Jupyter.
* **Root Cause:** Time-slicing shares memory. User A used 10GB, User B used 14GB, and the physical card only has 16GB. The GPU driver kills the process to save itself.
* **Fix:** You cannot enforce memory limits on the GPU hardware with time-slicing. You must enforce them at the **Container Level** (RAM) to discourage massive data loading, or move that user to a MIG profile.

=== Scenario B: "Replicas Not Showing Up"
* **Symptom:** You applied the ConfigMap, but `oc describe node` still shows `nvidia.com/gpu: 1`.
* **Fix:**
1. Check the Operator logs:
   `oc logs -n nvidia-gpu-operator -l app=gpu-operator-feature-discovery`
2. **Common Error:** The `ClusterPolicy` was not patched to reference the ConfigMap name.
3. **Common Error:** The ConfigMap key (`any` or `tesla-t4`) does not match the actual GPU label on the node.

== Phase 3: Troubleshooting MIG

=== Scenario C: The "Stuck Drain"
* **Symptom:** You applied the MIG label, but the node is stuck in `SchedulingDisabled` or `NotReady` for hours.
* **Root Cause:** The GPU Operator is trying to drain the node to repartition the hardware, but a Pod (usually a system pod or storage pod) is refusing to leave.
* **Fix:**
1. Check pending evictions: `oc get pods --all-namespaces --field-selector spec.nodeName=<node-name>`
2. Force the drain if necessary (use with caution in prod).

=== Scenario D: "Profile Mismatch"
* **Symptom:** The user selects the MIG Profile, but the Pod stays in `Pending`.
* **Error Message:** `0/10 nodes are available: 10 Insufficient nvidia.com/mig-1g.5gb.`
* **Root Cause:** A typo in the string.
* **The Reality Check:**
    * NFD advertises: `nvidia.com/mig-1g.5gb`
    * You requested: `nvidia.com/mig-1g.5gb` (Matches?)
    * *Watch out for memory variations:* Sometimes NFD sees `mig-1g.10gb` on newer A100 models. Always copy-paste from `oc describe node`.

== Summary of Commands

[cols="1,2", options="header"]
|===
|Goal |Command
|**Check Capacity** |`oc describe node <node> \| grep nvidia`
|**Check Labels** |`oc get node <node> --show-labels`
|**Restart NFD** |`oc delete pod -n nvidia-gpu-operator -l app=nfd-worker`
|**View Operator Logs** |`oc logs -f deployment/gpu-operator -n nvidia-gpu-operator`
|===

== Course Completion
Congratulations! You have successfully built, configured, and verified a **GPU-as-a-Service** allocation engine.

* **You learned:** How to stop "GPU Hoarding."
* **You built:** Time-Slicing for density and MIG for isolation.
* **You verified:** That the supply chain from Hardware to Dashboard is intact.

*Now, go optimize those clusters.*



++++======++++++========

= Module 4: The Traffic Controller (Governance with Kueue)
:navtitle: Governance & Quotas
:imagesdir: ../images
:toc: macro

*How to stop the "Fastest Finger First" race by implementing Fair Share Queuing.*

[abstract]
You have sliced your GPUs (Module 2) and partitioned your hardware (Module 3). But if you simply expose these to users, the first team to log in will consume everything. This module introduces **Kueue**, the traffic controller that manages quotas, priority, and fairness.

toc::[]

== The "Pinning" vs. "Queuing" Dilemma

Before you build your final Hardware Profile, you must make a critical architectural decision.

[cols="1,1", options="header"]
|===
|Strategy A: Static Pinning (The Old Way)
|Strategy B: Dynamic Queuing (The New Way)

|**Mechanism:** The Hardware Profile contains `nodeSelectors` and `tolerations` directly.
|**Mechanism:** The Hardware Profile contains a `LocalQueue` reference. The placement logic moves to a **ResourceFlavor**.

|**Behavior:** "I want this specific node." If full, the pod stays Pending forever.
|**Behavior:** "I want a resource like this." If full, the job is queued and prioritized.

|**Fairness:** None. First come, first served.
|**Fairness:** "Fair Share." Team A can borrow Team B's unused quota.
|===



== Step 0: Enabling the Service (Day 0 Config)
Kueue is installed but "dormant" by default. You must flip three switches to make the queuing features visible in the RHOAI Dashboard.

**1. Configure the Cluster State**
Ensure your `DataScienceCluster` resource has the Kueue component set to `Managed` or `Unmanaged` (if you need custom tuning).

**2. Enable the Dashboard UI**
You must tell the RHOAI Dashboard to show the "Queue" options in the menu.
[source,yaml]
----
apiVersion: opendatahub.io/v1alpha
kind: OdhDashboardConfig
metadata:
  name: odh-dashboard-config
  namespace: redhat-ods-applications
spec:
  # Enable the Queue selection UI
  disableKueue: false
----

**3. Label the Namespace**
Kueue only watches projects that are explicitly invited. You must label the user's namespace.
[source,bash]
----
oc label namespace <user-project> kueue.openshift.io/managed=true
----

== Step 1: Define the Resource Flavor (The Physical Layer)
In the Queuing model, the Hardware Profile becomes "dumb." It doesn't know about Taints or MIG labels. Instead, we define a **ResourceFlavor** object that holds the physical truth.

[source,yaml]
----
apiVersion: kueue.x-k8s.io/v1beta1
kind: ResourceFlavor
metadata:
  name: flavor-nvidia-mig-small
spec:
  nodeLabels:
    nvidia.com/mig-1g.5gb: "true"  # <1> The NFD Label
  tolerations:
  - key: "nvidia.com/gpu"          # <2> The Taint Key
    operator: "Exists"
    effect: "NoSchedule"
----
<1> **The Connection:** This connects the abstract flavor to the specific MIG slice you created in Module 3.
<2> **The Key:** This grants access to the Tainted node.

== Step 2: Define the Cluster Queue (The Policy Layer)
Now, we define the rules. Who gets what?

[source,yaml]
----
apiVersion: kueue.x-k8s.io/v1beta1
kind: ClusterQueue
metadata:
  name: cluster-queue-gpu-pool
spec:
  namespaceSelector: {} # Applies to all projects
  resourceGroups:
  - coveredResources: ["nvidia.com/mig-1g.5gb"]
    flavors:
    - name: flavor-nvidia-mig-small
      resources:
      - name: "nvidia.com/mig-1g.5gb"
        nominalQuota: 10  # <1>
        borrowingLimit: 5 # <2>
----
<1> **Guaranteed Quota:** The team is promised 10 slices.
<2> **Elasticity:** If the cluster is empty, they can borrow 5 more (Total 15).

== Step 3: Define the Local Queue (The Bridge)
Users cannot see the `ClusterQueue`. You must create a `LocalQueue` inside their project that acts as a connector.

[source,yaml]
----
apiVersion: kueue.x-k8s.io/v1beta1
kind: LocalQueue
metadata:
  namespace: <user-project>
  name: local-queue-gpu
spec:
  clusterQueue: cluster-queue-gpu-pool # Reference to Step 2
----

== Step 4: The Hardware Profile (The User Menu)
Finally, create the profile. When using Kueue, your Hardware Profile **must not** contain selectors.

* **Incorrect (Legacy):** Profile contains `tolerations: [{key: nvidia...}]`
* **Correct (Kueue):** Profile contains *only* the queue label.

[source,yaml]
----
apiVersion: dashboard.opendatahub.io/v1alpha1
kind: HardwareProfile
metadata:
  name: profile-mig-small-queue
  namespace: redhat-ods-applications
spec:
  displayName: "Small GPU Slice (Managed)"
  description: "Queued access to 1g.5gb slices."
  identifiers:
    - identifier: nvidia.com/mig-1g.5gb
      count: 1
  # CRITICAL: No nodeSelector or tolerations here!
  # The queuing system handles placement.
----

== Summary: The Full Supply Chain

1. **Physical:** You labeled the node `mig-1g.5gb` (Module 3).
2. **Logical:** You mapped that label to a `ResourceFlavor` (Module 4).
3. **Policy:** You assigned that flavor a Quota in the `ClusterQueue`.
4. **User:** The User selects the Profile, which submits a ticket to the Queue.

== Next Steps
Now that your governance layer is in place, proceed to **Module 5: Validation & Troubleshooting** to verify the end-to-end flow.



++++++++========++++++++=======


= Module 2b: The Heavy Lifters (Multi-GPU Aggregation)
:navtitle: Multi-GPU Aggregation
:imagesdir: ../images
:toc: macro

*How to bundle smaller cards (T4, L40S) to rival the power of an H100.*

[abstract]
Not every organization can afford a fleet of H100s. Often, you will find yourself with nodes full of smaller cards like NVIDIA T4s or L40S. This module teaches you the "Aggregation Strategy"—how to bundle multiple physical GPUs into a single Hardware Profile to power Large Language Model (LLM) fine-tuning and distributed training.

toc::[]

== Concept: The "Voltron" Strategy
Slicing (Module 2) and MIG (Module 3) are about taking a big resource and breaking it down. Aggregation is the opposite: taking small resources and combining them.

* **The Scenario:** A Data Scientist needs 64GB of VRAM to fine-tune Llama-3-70B.
* **The Problem:** You only have NVIDIA L40S cards (48GB VRAM each). A single card will crash with an OOM (Out of Memory) error.
* **The Solution:** You create a Hardware Profile that binds **2x L40S cards** together. The workload sees 96GB of total addressable VRAM (if using distributed libraries like Ray or PyTorch DDP).

== Step 1: The Basic "Bundle" Profile
The core mechanism for aggregation is simple: you ask for more than one.

In your Hardware Profile, you change the `count` parameter.

[source,yaml]
----
apiVersion: dashboard.opendatahub.io/v1alpha1
kind: HardwareProfile
metadata:
  name: nvidia-dual-l40s
  namespace: redhat-ods-applications
spec:
  displayName: "Dual L40S Station (96GB VRAM)"
  description: "Bundles 2 physical GPUs for larger model training."
  identifiers:
    - identifier: nvidia.com/gpu
      count: 2  # <1> The Bundle Request
  tolerations:
    - key: "nvidia.com/gpu"
      operator: "Exists"
      effect: "NoSchedule"
----
<1> **The Scheduler's Job:** Kubernetes will strictly look for a node that has *at least* 2 free GPUs. If a node only has 1 free, it will be skipped.

== Step 2: The Topology Trap (Critical Warning)
Asking for "2 GPUs" is dangerous if you don't care *which* 2 you get.

* **The Risk:** In some bare-metal servers, GPU 0 and GPU 1 might be on different PCIe switches (NUMA nodes). Communication between them is slow.
* **The Requirement:** For training, you need GPUs connected via **NVLink** or high-speed interconnects.
* **The Fix:** You must use `nodeSelectors` to pin this profile to machine types known to have good topology.

[source,yaml]
----
  # Inside the Hardware Profile spec
  nodeSelector:
    # Example: Pinning to a specific Dell or AWS machine type
    node.kubernetes.io/instance-type: "p5.48xlarge" 
    # OR using a custom label you applied to your high-performance racks
    hardware.topology/interconnect: "nvlink"
----

== Step 3: Aggregation with Kueue (Governance)
If you are using the Governance architecture (from Module 4), you do not put the `count` in the Hardware Profile. You put it in the **ResourceFlavor**.

This allows you to treat "Dual GPU" as a distinct class of service with its own quota.

**1. Define the Flavor (The Physical Bundle)**
[source,yaml]
----
apiVersion: kueue.x-k8s.io/v1beta1
kind: ResourceFlavor
metadata:
  name: flavor-dual-gpu
spec:
  nodeLabels:
    nvidia.com/gpu.count: "2" # <1> Target nodes with 2 cards
  tolerations:
  - key: "nvidia.com/gpu"
    operator: "Exists"
----

**2. Define the Profile (The Logical Pointer)**
[source,yaml]
----
apiVersion: dashboard.opendatahub.io/v1alpha1
kind: HardwareProfile
metadata:
  name: profile-dual-gpu-queue
spec:
  displayName: "Dual GPU Training (Queued)"
  identifiers:
    - identifier: nvidia.com/gpu
      count: 2 # <1>
----
<1> **Note:** Even with Kueue, passing `count: 2` here helps the dashboard validate the request, but the heavy lifting of admission checks is done by the ClusterQueue limits.

== Lab: Create a "Heavy Lifter"
1. **Identify Multi-GPU Nodes:** Run `oc get nodes -L nvidia.com/gpu.count` to find nodes with >1 GPU.
2. **Create the Profile:** Define a `HardwareProfile` requesting `count: 2`.
3. **Launch a Workbench:** Select the profile.
4. **Verify inside the Pod:** Open a terminal in Jupyter and run:
   `nvidia-smi`
   *Result:* You should see two distinct GPU devices listed (Device 0 and Device 1).

== Summary: The "Scaling Up" Strategy

[cols="1,1", options="header"]
|===
|Strategy |Scaling Down (Slicing/MIG) |Scaling Up (Aggregation)
|**Goal** |Efficiency (ROI) |Power (Performance)
|**Mechanism** |1 GPU -> Many Users |Many GPUs -> 1 User
|**Key Param** |`identifiers` (mig-1g.5gb) |`count` (2, 4, 8)
|**Ideal For** |Notebooks, Dev, Inference |LLM Fine-tuning, Distributed Training
|===